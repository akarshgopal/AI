{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from atlas_ml import *\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([i for i in range(10)]).reshape(10,1)\n",
    "Y = np.array([i for i in range(10)]).reshape(10,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing from: https://www.tensorflow.org/tutorials/sequences/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'Shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text_as_int[:6347700].reshape(100, text_as_int.shape[0]//100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(data[:-1,:], axis=-1)\n",
    "Y = np.expand_dims(data[1:,:], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rec_layer():\n",
    "    def __init__(self, n_prev, n_next, activation):\n",
    "        self.WA = init_matrix(n_prev, n_next, activation)\n",
    "        self.WH = init_matrix(n_next, n_next, activation)\n",
    "        self.B = init_matrix(1, n_next, activation)\n",
    "        self.activation = activation()\n",
    "        \n",
    "        self.dWA = np.zeros(self.WA.shape)\n",
    "        self.dWH = np.zeros(self.WH.shape)\n",
    "        self.dB = np.zeros(self.B.shape)\n",
    "    \n",
    "        self.V_dWA = np.zeros(self.WA.shape)\n",
    "        self.V_dWH = np.zeros(self.WH.shape)\n",
    "        self.V_dB = np.zeros(self.B.shape)\n",
    "        \n",
    "    def forward(self, A0 , H0):\n",
    "        self.Z = np.einsum('ln,ml-> mn', self.WA, A0) + np.einsum('ln,ml-> mn', self.WH, H0) +  self.B\n",
    "        self.A = self.activation.activate(self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    def grad(self, dA, dH, A0, H0, m):\n",
    "        dAdZ = self.activation.diff(self.Z)\n",
    "        self.dZ = np.multiply(dA + dH , dAdZ)\n",
    "        \n",
    "        self.dWA = (1./m)*np.einsum('mn,ml->ln',self.dZ, A0)\n",
    "        self.dWH = (1./m)*np.einsum('mn,ml->ln',self.dZ, H0)\n",
    "        self.dB = (1./m)*(np.einsum('mn->n',self.dZ))\n",
    "        \n",
    "        dA_prev = np.einsum('ln, mn->ml',self.WA, self.dZ)\n",
    "        dH_prev = np.einsum('ln, mn->ml',self.WH, self.dZ)        \n",
    "        return dA_prev, dH_prev\n",
    "    \n",
    "    def out_grad(self, dZ, dH, A0, H0, m):\n",
    "        dAdZ = self.activation.diff(self.Z)\n",
    "        self.dZ = dZ + np.multiply(dH , dAdZ)\n",
    "        self.dWA = (1./m)*np.einsum('mn,ml->ln',self.dZ, A0)\n",
    "        self.dWH = (1./m)*np.einsum('mn,ml->ln',self.dZ, H0)\n",
    "        self.dB = (1./m)*(np.einsum('mn->n',self.dZ))\n",
    "        \n",
    "        dA_prev = np.einsum('ln, mn->ml',self.WA, self.dZ) \n",
    "        dH_prev = np.einsum('ln, mn->ml',self.WH, self.dZ) \n",
    "        return dA_prev, dH_prev\n",
    "        \n",
    "    def step(self, lr, beta):\n",
    "        self.V_dWA = (beta * self.V_dWA + (1. - beta) * self.dWA)\n",
    "        self.V_dWH = (beta * self.V_dWH + (1. - beta) * self.dWH)\n",
    "        self.V_dB = (beta * self.V_dB + (1. - beta) * self.dB)\n",
    "        self.WA = self.WA - lr*self.V_dWA\n",
    "        self.WH = self.WH - lr*self.V_dWH\n",
    "        self.B = self.B - lr*self.V_dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BookKeeping gains dimension in Time and therefore definition of RNN requires use of lists and for loops. Using these on top of the atlas_ml NN architure adds significant overheads making training extremely slow compared to A.Karpathy's Vanilla RNN(presented at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, X_size, Y_size, lossfn):\n",
    "        self.n_hidden = 100\n",
    "        self.L1 = rec_layer(X_size, self.n_hidden, tanh)\n",
    "        self.L2 = layer(self.n_hidden, Y_size, softmax)\n",
    "        self.lossfn = lossfn()\n",
    "        self.Y_size = Y_size\n",
    "        \n",
    "    def clear_book(self, batch_size=4):\n",
    "        self.total_loss = 0\n",
    "        self.loss = 0\n",
    "        self.L1Z = list()\n",
    "        self.L1H = [np.zeros([batch_size, self.n_hidden])]\n",
    "        self.L2Z = list()\n",
    "        self.L2H = list()\n",
    "        \n",
    "        self.L2dW = np.zeros(self.L2.W.shape)\n",
    "        self.L2dB = np.zeros(self.L2.B.shape)\n",
    "        self.L1dWH = np.zeros(self.L1.WH.shape)\n",
    "        self.L1dWA = np.zeros(self.L1.WA.shape)\n",
    "        self.L1dB = np.zeros(self.L1.B.shape)\n",
    "    \n",
    "    def generate(self, prompt, seq_len):\n",
    "        self.clear_book()\n",
    "        seq = [one_hot(np.array([[char2idx[prompt]]]), self.Y_size)]\n",
    "        L1H = np.zeros([1, self.n_hidden])\n",
    "        for i in range(seq_len):\n",
    "            L1H = self.L1.forward(seq[i], L1H)\n",
    "            seq.append(self.L2.forward(self.L1.A))\n",
    "        return seq\n",
    "    \n",
    "    def f_pass(self, X):\n",
    "        for i in range(X.shape[0]):\n",
    "            self.L1H.append(self.L1.forward(one_hot(X[i],self.Y_size), self.L1H[i]))\n",
    "            self.L1Z.append(self.L1.Z)\n",
    "            \n",
    "            self.H = self.L2.forward(self.L1.A)\n",
    "            self.L2H.append(self.H)\n",
    "            self.L2Z.append(self.L2.Z)\n",
    "        #self.L1H[-1]=np.zeros([batch_size, self.n_hidden])\n",
    "        return np.array([inv_one_hot(i) for i in self.L2H])\n",
    "    \n",
    "    def back_prop(self, X, Y, batch_size,reg_lambda=0):\n",
    "        HtdH = np.zeros([batch_size, self.n_hidden])\n",
    "        for i in range(X.shape[0]-1,-1,-1):\n",
    "            self.L2.Z = self.L2Z[i]\n",
    "            self.L1.Z = self.L1Z[i]\n",
    "            \n",
    "            m = batch_size\n",
    "            Y2 = one_hot(Y[i],self.Y_size)\n",
    "\n",
    "            self.loss = self.lossfn.get_loss(self.L2H[i],Y2)\n",
    "            self.total_loss += self.loss\n",
    "            \n",
    "            L2dZ = self.lossfn.diff(self.L2H[i],Y2)\n",
    "            \n",
    "            L2dH = self.L2.out_grad(L2dZ, self.L1H[i], m)\n",
    "            _, HtdH = self.L1.grad(L2dH, HtdH, one_hot(X[i],self.Y_size), self.L1H[i+1], m) \n",
    "            \n",
    "            self.L2dW += self.L2.dW \n",
    "            self.L2dB += self.L2.dB\n",
    "            self.L1dWH += self.L1.dWH\n",
    "            self.L1dWA += self.L1.dWA\n",
    "            self.L1dB += self.L1.dB\n",
    "\n",
    "        self.L2.dW = np.clip(self.L2dW, -5, 5, out=self.L2dW) \n",
    "        self.L2.dB = np.clip(self.L2dB, -5, 5, out=self.L2dB)\n",
    "        self.L1.dWH = np.clip(self.L1dWH, -5, 5, out=self.L1dWH)\n",
    "        self.L1.dWA = np.clip(self.L1dWA, -5, 5, out=self.L1dWA)\n",
    "        self.L1.dB = np.clip(self.L1dB, -5, 5, out=self.L1dB)\n",
    "    \n",
    "    def optim(self, lr, beta=0):\n",
    "        self.L1.step(lr,beta)\n",
    "        self.L2.step(lr,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 82)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [one_hot(np.array([[char2idx['I']]]), len(vocab))]\n",
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPTT(batch_size,X,Y, model,lr,beta,reg_lambda=0):\n",
    "    m = np.shape(X)[1]\n",
    "    H = np.zeros(X.shape)\n",
    "    for i in range(0,m-m//batch_size,batch_size):\n",
    "        X_batch = X[:,i:i+batch_size]\n",
    "        Y_batch = Y[:,i:i+batch_size]\n",
    "        model.clear_book(batch_size)\n",
    "        H[:,i:i+batch_size]= model.f_pass(X_batch)\n",
    "        model.back_prop(X_batch, Y_batch, batch_size, reg_lambda)\n",
    "        model.optim(lr, beta) \n",
    "    O = H.flatten()\n",
    "    L = Y.flatten()\n",
    "    tr_acc = model_accuracy(O,L)    \n",
    "    return model.loss, tr_acc, O, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = len(vocab)\n",
    "n_in = len(vocab)\n",
    "rnn = RNN(n_in, n_out, CE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.clear_book(400)\n",
    "H = rnn.f_pass(X[:,:400,:])\n",
    "O = H.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "lr_decay = 1\n",
    "\n",
    "data_size = X.shape[1]\n",
    "\n",
    "beta = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3386385586021976, 0.06508234987841774)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, tr_acc, O, L = BPTT(16,X,Y,rnn,lr, beta)\n",
    "loss, tr_acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". .la a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  st et a st ehs a t  s\n"
     ]
    }
   ],
   "source": [
    "seq = rnn.generate('.',1000)\n",
    "print(''.join([idx2char[int(inv_one_hot(i))] for i in seq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RYsRVVVRRORbsRRRYYVYRsVYORbsRRVVQYRbQVVQbVROYbbVfsVbYVVVYYbVbRQfR.VYfRYbVVVRRYRVYR'RRRYVVRsVRRYYsRY_YsR'RRRR'YsVRsVRR3sYYYVsVRRRYYRRRVRYRYfQVVRYVRRRVRs_VRbYRRVYRVRYYVRR.VYRRRsOVRbRVRVVRYVYRRVR3VYRYbVVRRYbRRRYsVRYRRRYYOVRfVYRQYRRYQRYR3VYRYbVVVRROYsbVYYVROVYRbYRRRRRRYVRRRYYRVffRR.RYRYfQVVRYVRRRVRVRYsVRbR.V.YYRbYYROVYRbYRRR.RRYVRR.YRRVffR..R..YfQVV.RVRRVR.VYsVROVYRbRRRR.RRYVRR.YRRV..R..R....Q...RVVRRRR_.YsOORRRR.R_O.RRR.VRV.RYVYRR.R3VYRYbVVRRYsRs.VVRbR.VRR.VY.VbRRs..RV.RVYR.YRRYVR...RRV..VRRVR.....R.RRRR..s..sRVRR..V.V.RYb..YRV.RR.RRYVR..Y.YVVRRR..Rs..V.R.V..Y....R..YR...V.YR..R.YVR..Y.YVRR.V..V.V.R....Y....R..YR.....YR..R.Y.R....YVVRR......R.........R.........R...R.R......R...R..........RRVV......V..........R......R.........R..........VV...R..R.....RR.V..........R...R.......R.............R........R...R.................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ''.join(idx2char[int(i)] for i in O[:1500])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1770,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, Y, X_test, Y_test, metric, n_epochs=10, \\\n",
    "    batch_size=4, lr=0.0003, lr_decay=1, beta=0, reg_lambda=0, show_test_acc=True, shuffle_data=True):\n",
    "    data_size = X.shape[0]\n",
    "    for e in range(n_epochs):\n",
    "        #shuffle dataset\n",
    "        np.random.seed(138)\n",
    "\n",
    "        if shuffle_data:\n",
    "            shuffle_index = np.random.permutation(data_size)\n",
    "            X, Y = X[shuffle_index,:], Y[shuffle_index,:]\n",
    "\n",
    "        #SGD with momentum\n",
    "        model.loss=0\n",
    "        model.total_loss=0\n",
    "        loss, tr_acc, O, L = BPTT(batch_size,X,Y,model,lr, beta)\n",
    "        lr = lr*lr_decay\n",
    "        s = ''.join(idx2char[int(i)] for i in O[:1500])\n",
    "        print(s)\n",
    "        if show_test_acc:\n",
    "            m = np.shape(X_test)[0]\n",
    "            H = np.zeros(Y_test.shape)\n",
    "            for i in range(0,m,batch_size):\n",
    "                X_test_batch = X_test[i:i+batch_size]\n",
    "                H[i:i+batch_size] = model.f_pass(X_test_batch)\n",
    "            O = inv_one_hot(H)\n",
    "            L = inv_one_hot(Y_test)\n",
    "            acc = metric(O,L)\n",
    "        else:\n",
    "            acc = 0\n",
    "            \n",
    "        plt.plot(e,tr_acc, 'bo')\n",
    "        plt.plot(e,acc,'ro')\n",
    "        plt.plot(e,loss,'gx')\n",
    "        clear_output()\n",
    "        print(f\"epoch:{e+1}/{n_epochs} | Loss:{loss:.4f} \\\n",
    "Train Accuracy: {tr_acc:.4f} | Test_Accuracy:{acc:.4f}\")\n",
    "        \n",
    "    #plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1846,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6/10 | Loss:0.4423 Train Accuracy: 0.2071 | Test_Accuracy:0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1846-01e87d856940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train(rnn, X, Y, X, Y, model_accuracy, n_epochs, \\\n\u001b[0;32m----> 2\u001b[0;31m     batch_size, lr, lr_decay, beta, show_test_acc=False,shuffle_data=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1770-7c8bc392aab7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X, Y, X_test, Y_test, metric, n_epochs, batch_size, lr, lr_decay, beta, reg_lambda, show_test_acc, shuffle_data)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBPTT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlr_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx2char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1837-c4540844cec7>\u001b[0m in \u001b[0;36mBPTT\u001b[0;34m(batch_size, X, Y, model, lr, beta, reg_lambda)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_book\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1835-d66bc3d0becc>\u001b[0m in \u001b[0;36mback_prop\u001b[0;34m(self, X, Y, batch_size, reg_lambda)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mL2dH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL2dZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1H\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHtdH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL2dH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHtdH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1H\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL2dW\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1638-931be097f303>\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self, dA, dH, A0, H0, m)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mn->n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mdA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ln, mn->ml'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mdH_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ln, mn->ml'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdH_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/onlinepub/lib/python3.6/site-packages/numpy/core/einsumfunc.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;31m# If no optimization, run pure einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptimize_arg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0mvalid_einsum_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'order'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'casting'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(rnn, X, Y, X, Y, model_accuracy, n_epochs, \\\n",
    "    batch_size, lr, lr_decay, beta, show_test_acc=False,shuffle_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_one_hot(rnn.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referred to A.Karpathy's Vanilla RNN for inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('Shakespeare.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[10], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ) )\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "n += 1 # iteration counter "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onlinepub",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
