{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy tqdm progress bar\n",
    "for i in tqdm.tqdm(np.arange(100)):\n",
    "    time.sleep(0.01)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: The Data\n",
    "\n",
    "For Basic classification we just use Kaggle's Spaceship Titanic dataset. It's a simple dataset with a few features and a binary label.\n",
    "Download from: https://www.kaggle.com/competitions/spaceship-titanic/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data briefly, see summary, histograms, ranges, correlations, etc.\n",
    "\n",
    "The goal here is to get a feel for the data, and to see if there are any obvious issues with it.\n",
    "Also we prepare the data for learning by doing some basic preprocessing like assigning numerical labels to categorical columns, removing NaNs, imputing values if needed, and normalizing ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/spaceship-titanic/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a common sense POV, passengerID shouldn't affect the passenger's survival, unless it's a proxy for some feature that isn't in the dataset. So we won't drop it just yet.\n",
    "\n",
    "Our 0 / 1 classification label here is Transported. Our objective is to predict whether a passenger was transported or not given the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for categorical columns show set of unique values \n",
    "train_df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical data to numerical integer codes\n",
    "corr_df = train_df.copy()\n",
    "for col in corr_df.columns:\n",
    "    if corr_df[col].dtype == 'object' or 'bool':\n",
    "        corr_df[col] = corr_df[col].astype('category').cat.codes\n",
    "    else:\n",
    "        # normalize data by column to -1 to 1 per column\n",
    "        corr_df[col] = (corr_df[col] - corr_df[col].mean()) / corr_df[col].std()\n",
    "\n",
    "print(corr_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "correlation_matrix = corr_df.corr(method='pearson')\n",
    "# plot heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CryoSleep seems to be decently correlated with Transported, so we should expect it to be a major feature in our model.\n",
    "RoomService seems to be negatively correlated with Transported, so we should expect it to be a major feature in our model.\n",
    "\n",
    "Anyway, we're not hand engineering stuff here. The goal is to try out some basic classic ML approaches for classification and see how well they work. Less goo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / val split 80:20 randomly\n",
    "df = corr_df.copy()\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate an array of random indices for shuffling\n",
    "indices = np.arange(len(df))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(0.8 * len(df))\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df_split = df.iloc[indices[:split_index]]\n",
    "val_df = df.iloc[indices[split_index:]]\n",
    "\n",
    "# Reset the index in the resulting DataFrames\n",
    "train_df_split.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Get train df splits labels and val df's labels\n",
    "train_labels = train_df_split['Transported']\n",
    "val_labels = val_df['Transported']\n",
    "train_df_split=train_df_split.drop(columns=['Transported'])\n",
    "val_df=val_df.drop(columns=['Transported'])\n",
    "\n",
    "# Print the shapes of the resulting DataFrames\n",
    "print(\"Train set shape:\", train_df_split.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n",
    "print(\"train_labels\", train_labels.shape)\n",
    "print(\"val_labels\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Ol' Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: K Nearest Neighbours\n",
    "\n",
    "K Nearest Neighbours works by classifying a given point in an N-D space based on its k nearest neighbours based on some distance metric, usually a euclidian metric, i.e. L2-norm. This might be problematic with categorical features because how exactly does distance play a part there? for e.g. the planet category is earth, trappist etc. putting these on a numerical axis linearly makes little sense.. but let's see how our approaches handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k nearest neighbours\n",
    "class KNNClassifier:\n",
    "    def __init__(self, X_train: pd.DataFrame, y_train: pd.DataFrame, k=3, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def _calculate_distance(self, point1, point2):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(point1 - point2))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "\n",
    "        for test_point in X_test:\n",
    "            # Calculate distances to all training points\n",
    "            distances = np.array([self._calculate_distance(test_point, train_point) for train_point in self.X_train])\n",
    "\n",
    "            # Get indices of k-nearest neighbors\n",
    "            k_nearest_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "            # Get the corresponding labels of k-nearest neighbors\n",
    "            k_nearest_labels = self.y_train[k_nearest_indices]\n",
    "\n",
    "            # Find the most common class among the k-nearest neighbors\n",
    "            predicted_label = np.argmax(np.bincount(k_nearest_labels))\n",
    "\n",
    "            predictions.append(predicted_label)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train, y_train, X_test are NumPy arrays\n",
    "# Replace them with your actual training features, training labels, and test features\n",
    "\n",
    "# Create and train the KNN classifier with Manhattan distance\n",
    "knn_classifier_manhattan = KNNClassifier(train_df_split, train_labels, k=3, distance_metric='euclidean')\n",
    "\n",
    "# Make predictions\n",
    "predictions_manhattan = knn_classifier_manhattan.predict(val_df)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions with Manhattan distance:\", predictions_manhattan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Support Vector Machines\n",
    "\n",
    "Basic operating principle: Draw a hyperplane between the points that maximises sum of distances of the points from the hyperplane. Applying kernel functions such as radial basis functions or polynomials can increase the complexity of seperation. Picture it as embedding the data points in a more complicated manifold and cutting it across with a hyperplane to seperate the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3a: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
