{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: The Data\n",
    "\n",
    "For Basic classification we just use Kaggle's Spaceship Titanic dataset. It's a simple dataset with a few features and a binary label.\n",
    "Download from: https://www.kaggle.com/competitions/spaceship-titanic/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data briefly, see summary, histograms, ranges, correlations, etc.\n",
    "\n",
    "The goal here is to get a feel for the data, and to see if there are any obvious issues with it.\n",
    "Also we prepare the data for learning by doing some basic preprocessing like assigning numerical labels to categorical columns, removing NaNs, imputing values if needed, and normalizing ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/spaceship-titanic/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a common sense POV, passengerID shouldn't affect the passenger's survival, unless it's a proxy for some feature that isn't in the dataset. So we won't drop it just yet.\n",
    "\n",
    "Our 0 / 1 classification label here is Transported. Our objective is to predict whether a passenger was transported or not given the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical data to numerical integer codes\n",
    "corr_df = train_df.copy()\n",
    "for col in corr_df.columns:\n",
    "    if corr_df[col].dtype == 'object' or 'bool':\n",
    "        corr_df[col] = corr_df[col].astype('category').cat.codes\n",
    "    else:\n",
    "        # normalize data by column to -1 to 1 per column\n",
    "        corr_df[col] = (corr_df[col] - corr_df[col].mean()) / corr_df[col].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CryoSleep seems to be decently correlated with Transported, so we should expect it to be a major feature in our model.\n",
    "RoomService seems to be negatively correlated with Transported, so we should expect it to be a major feature in our model.\n",
    "\n",
    "Anyway, we're not hand engineering stuff here. The goal is to try out some basic classic ML approaches for classification and see how well they work. Less goo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / val split 80:20 randomly\n",
    "df = corr_df.copy()\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate an array of random indices for shuffling\n",
    "indices = np.arange(len(df))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(0.8 * len(df))\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df_split = df.iloc[indices[:split_index]]\n",
    "val_df = df.iloc[indices[split_index:]]\n",
    "\n",
    "# Reset the index in the resulting DataFrames\n",
    "train_df_split.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Get train df splits labels and val df's labels\n",
    "train_labels = train_df_split['Transported']\n",
    "val_labels = val_df['Transported']\n",
    "train_df_split=train_df_split.drop(columns=['Transported'])\n",
    "val_df=val_df.drop(columns=['Transported'])\n",
    "\n",
    "# Print the shapes of the resulting DataFrames\n",
    "print(\"Train set shape:\", train_df_split.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n",
    "print(\"train_labels\", train_labels.shape)\n",
    "print(\"val_labels\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's go to the Zoo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Feed Forward Neural Network\n",
    "\n",
    "Best reference to start: https://github.com/karpathy/nn-zero-to-hero\n",
    "Ofc also fast ai: https://course.fast.ai/Lessons/lesson3.html\n",
    "Andrew Ng's deep learning specialization is also a good resource and recommmended for a more thorough understanding of the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)  # Set the seed for reproducibility\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "input_size = train_df_split.shape[1]\n",
    "hidden_size = 60\n",
    "num_classes = 2\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "# Convert train_df_split and train_labels to tensors\n",
    "train_df_split_tensor = torch.tensor(train_df_split.values, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Iterate over the data for a specified number of epochs\n",
    "num_epochs = 800\n",
    "losses = []\n",
    "for epoch in tqdm.tqdm(range(num_epochs), total=num_epochs, desc=\"Epochs\"):\n",
    "    # Forward pass\n",
    "    outputs = model(train_df_split_tensor)\n",
    "    loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss for every 10th epoch\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Divide the learning rate by 2 if epoch == 400\n",
    "    if epoch%100 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] /= 1.5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses vs epoch\n",
    "plt.plot(range(1, num_epochs+1), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Losses vs Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation on val_df\n",
    "val_df_tensor = torch.tensor(val_df.values, dtype=torch.float32)\n",
    "\n",
    "predictions = model.forward(val_df_tensor)\n",
    "predicted_classes = torch.argmax(predictions, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predicted_classes.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Accuracy:\n",
    "We just subtract predictions from the labels, take the absolute value and calculate the mean. Subtracting this mean from 1 should give us a 0-1 accuracy metric. i.e 1 implies 100%, 0 implies 0%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 1-abs(predictions - val_labels).mean()\n",
    "\n",
    "print(\"accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic NN with 1 hidden layer does as well as an SVM, and with some hyperparam tuning can get upto 0.75!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix is a visualisation of how many true positives, false positives, true negatives and false negatives our model predicts.\n",
    "It is useful for getting a sense of how wrong our model is, and where it might be going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "def plot_confusion_matrix(pred, labels):\n",
    "    conf_matrix = np.zeros((2, 2))\n",
    "    for i in range(len(labels)):\n",
    "        conf_matrix[labels[i], pred[i]] += 1\n",
    "\n",
    "    # Plot the confusion matrix using seaborn\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix Manhattan')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(predictions, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do deeper NNs, but let's stop here for the titanic problem. larger networks need more data to git gud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for some more interesting data:\n",
    "\n",
    "For image classification we'll use the Fashion MNIST dataset as a nod to Try-it-on ;)\n",
    "Download from: https://www.kaggle.com/datasets/zalando-research/fashionmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Deep NNs: Let's go Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: CNNs: Let's give NNs Sight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: RNNS: Let's let NNs remember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a: LSTMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b: GRUs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
