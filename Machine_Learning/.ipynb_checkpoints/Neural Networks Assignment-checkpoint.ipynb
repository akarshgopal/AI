{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 6: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks (NN) are essentially the closest we've come to universal function approximators. All the hype you've been hearing about Deep Learning and \"AI\" are due to advances in NNs. These beasts deserve a separate course, but here you'll get to learn how a basic NN works\n",
    "\n",
    "If you recall logistic regression, we model a classification problem as a non-linear function of the input vector.\n",
    "When the problem in question is quite complicated, and the number of possible input features are large, simple classification, regression or classic ML techniques have limited ability. Composing functions of functions, say $f( g( h(X,W_{0}),W_{1}),W_{2}) $ would be better equipped to 'learn' more complicated functions. \n",
    "\n",
    "NNs are effectively compositions of functions consisting of layers of 'nodes'. The permuted connections between these nodes allow complex 'functions' to be 'learnt'. Depicted below is a 3-layer NN, consisting of an input layer, a 'hidden' layer and an output layer.\n",
    "\n",
    "Why provide an elaborate explanation when this beautiful visualization exists:\n",
    "https://www.youtube.com/watch?v=aircAruvnKk ?\n",
    "\n",
    "If the following 2 code blocks throw an error, it is because you are missing \"nn.png\" and \"nnlogic.png\" files which you can download from canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"nn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To start off, let's first visualise logistic regression in terms of a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"nnlogic.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a 2-layer NN, which is effectively a logistic regression model. The input layer is X and output layer is a single node y. The connections from the input layer to the output layer represent the weights that are to be 'learnt', and represented as a matrix W such that sigmoid(W*X) = y\n",
    "\n",
    "So a 3-Layer NN with a h-neuron hidden layer would essentially be h ~logistic regressions of the input times y ~logistic regressions of the hidden layer. So there would be x*h + h*w weights to be learnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "The computation of the output layer by composing functions through each hidden layer is called forwards propagation. These functions are called 'activations' and are non-linear in nature. Let's say we have an input feature vector X, a single hidden layer witha sigmoid activation and an output layer with a sigmoid activation.\n",
    "\n",
    "A forward-pass is then given by:\n",
    "\n",
    "$ Y' = \\sigma( W_{2}.( \\sigma( W_{1}.X)))$\n",
    "\n",
    "Where $W_{1}$ and $W_{2}$ are the weights/parameters to be learnt, $X$ is the input vector and $Y'$ is the output vector of the NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Propagation\n",
    "\n",
    "Remember how we optimised the model in logistic regression? We used the partial differentials of the Cost function w.r.t the weight we were trying to optimize. Same concept, but because we have a composition of functions, we use the most important rule in differentiation, chain rule. So let's say cost function $J(x,W_{1},W_{2}, Y) = f( g(x,W_{1}),W_{2},Y) $. \n",
    "Then $  \\frac{\\partial J}{\\partial W_{1}} = \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial W_{1}} $\n",
    "\n",
    "The optimising step would then be $ W_{1} \\rightarrow W_{1} - \\alpha \\cdot \\frac{\\partial J}{\\partial W_{1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to get a better grasp of how NNs work, let's build one from scratch using numpy and python\n",
    "\n",
    "This tutorial uses python classes in order to build a kind of NN-library. This is to help us breakdown the different components that go into a NN and then be able to build different architectures of NN more easily. \n",
    "\n",
    "This also allows us to peek into the inner workings of the NN since we can print the weights between any two layers to understand what is happening there. \n",
    "\n",
    "You are required to complete the code where it says # INSERT CODE HERE #\n",
    "\n",
    "It is advised to run 'unit' tests on each block you finish, where possible, by perhaps using a random array.\n",
    "\n",
    "Note: The dimensioning of data is as follows: nxm, n =number of features, m = number of examples. This is the transposed version of what was being used for the previous assignments.\n",
    "\n",
    "### A quick primer on python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object:                    # This is a class, it can have 'members' that are functions or data\n",
    "    def __init__(self,x):        # the __init__ function initialises the class Object whenever a object is declared\n",
    "        self.A = x               # here, a data member A (accessed as self.A) is assigned x whenever an object is created\n",
    "    def mem_function(self,X):    # this is a member function, it can access the class's data and use external parameters\n",
    "        self.B = self.A + X      # here another data member is created and assigned a value\n",
    "        \n",
    "obj = Object(2)                  # here an Object object is created as obj\n",
    "print(\"object \", obj)\n",
    "print(\"it's A value \",obj.A)\n",
    "obj.mem_function(3)              # here the member function is called using <class_name>.<functoin_name>\n",
    "obj.B                            # this is how the object's data can be accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The below functions are trivial and can be skipped, unless, you want to come up with a cleaner way to do them ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "    \n",
    "def flatten_imgs(X):\n",
    "    n_img = np.shape(X)[0]\n",
    "    h = np.shape(X)[1]\n",
    "    w = np.shape(X)[2]\n",
    "    size_arr = h*w\n",
    "    return np.reshape(X,(n_img,size_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding \n",
    "\n",
    "One hot encoding is basically representing a class as a boolean vector: e.g if we have 3 classes (let's say classes 1,2,3 ) and we want to represent class '1'. We would do this as [1 0 0]$^{T}$. Class '2' would be [0 1 0]$^{T}$ and so on.. \n",
    "\n",
    "One hot encoding allows us to output a probability distribution vector [p1 p2 p3]$^{T}$ such that these probabilities represent the likelihood of the particular class. So a vector [0.9 0.02 0.08]$^{T}$ would mean that the output represents class 1 with a 0.9 probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y, n_class):\n",
    "    #accept a (1xm) 'label' vector, number of classes  \n",
    "    #and return a nxm 'one-hot' matrix\n",
    "    \n",
    "    # INSERT CODE HERE #\n",
    "    \n",
    "    return O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to have the inverse encoding from on-hot representation. Thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_one_hot(O):\n",
    "    # do the inverse of the one_hot function  \n",
    "    \n",
    "    # INSERT CODE HERE #\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can quickly get messy if the input data is not normalized. You could try implementing the NN without normalizing the data and find out how it affects the performance.\n",
    "\n",
    "There is one common way of normalizing data, namely by computing the mean $\\mu$ and standard devitation $\\sigma$\n",
    "<ul> \n",
    "    $\\displaystyle\\mu = \\frac{1}{M}\\sum_{m=0}^{M-1} X_m$,  $\\displaystyle ~~~~\\sigma = \\sqrt{\\frac{1}{M}\\sum_{m=0}^{M-1} (X_m -\\mu)^2}$\n",
    "</ul>\n",
    "and then defining \n",
    "<ul> \n",
    "    $\\displaystyle Y_m = \\frac{X_m - \\mu}{\\sigma}$  $~~~~ m = 0, \\ldots, M-1$\n",
    "</ul>\n",
    "In this case the new data points $Y_m$ are centered around value $0$ with a spread $1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    # normalize the matrix X using the (mean,std) normalization method\n",
    "    # you can use numpy's inbuilt mean and std functions, but remember to specify axis!\n",
    "    \n",
    "    # INSERT CODE HERE #\n",
    "    \n",
    "    return N_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to effectively measure the performance of our model we can define a 'metric'. Here we use a straight-forward 'right or not' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(H,Y):\n",
    "    # H and Y have to be one-hot matrices\n",
    "    # H is the output of the model (a.k.a Hypothesis)\n",
    "    n = np.shape(H)[1]\n",
    "    err = 0\n",
    "    O = inv_one_hot(H)\n",
    "    L = inv_one_hot(Y)\n",
    "    for i in range(n):\n",
    "        if O[0,i]!=L[0,i]:\n",
    "            err += 1\n",
    "    accuracy = (1 - err/n)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Each node except those in the input layer represents an activation function, which applies a 'non-linearity' to its input parameter.\n",
    "\n",
    "ML Researchers have empirically found several different activation functions to work with their respective pros and cons. Listed below are the most popular.\n",
    "Softmax is a unique activation function in that it is used in multiclass classification i.e for categorical data.\n",
    "\n",
    "For some more comprehension: https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Activation|Equation|\n",
    "|----|---------|\n",
    "|<img width=50/>|<img width=100/>|\n",
    "|Sigmoid| $ \\sigma(x) = \\frac{1}{1+e^{-x}} $ |\n",
    "| Tanh | $tanh(x)$ |\n",
    "| ReLu | $ max(0,x)$    |\n",
    "| Softmax| $ \\frac{e^{x_{i}}}{\\sum e^{x_{i}}} $ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    def activate(Z):\n",
    "        #compute A = sigmoid(Z)\n",
    "        \n",
    "        # INSERT CODE HERE #\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def diff(self,Z):\n",
    "        #dsig = diff(sigmoid(Z))\n",
    "        \n",
    "        # INSERT CODE HERE #\n",
    "        \n",
    "        return dsig\n",
    "    \n",
    "class tanh:\n",
    "    def activate(Z):\n",
    "        #compute A = tanh(Z)\n",
    "        \n",
    "        # INSERT CODE HERE #\n",
    "        \n",
    "        return A\n",
    "\n",
    "    def diff(self,Z):\n",
    "        #dsig = diff(tanh(Z))\n",
    "        \n",
    "        # INSERT CODE HERE #\n",
    "        \n",
    "        return d_tanh\n",
    "    \n",
    "class relu:\n",
    "    def activate(Z):\n",
    "        #compute A = relu(Z)\n",
    "        \n",
    "        # INSERT CODE HERE #\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def diff(self,Z):\n",
    "        #dsig = diff(relu(Z))\n",
    "        \n",
    "        # INSERT CODE HERE #\n",
    "        \n",
    "        return d_rel\n",
    "    \n",
    "# let's leave softmax :)\n",
    "class softmax:\n",
    "    def activate(Z):\n",
    "        e_Z = np.exp(Z- np.max(Z,axis=0))\n",
    "        return e_Z / e_Z.sum(axis=0)\n",
    "\n",
    "# Just for formality. Not recommended to be used. \n",
    "# w.r.t Z is almost never needed since we compute C.E loss derivative w.r.t Z directly viz  H-Y\n",
    "    def diff(self,Z):\n",
    "        sftmx = self.activate(Z)\n",
    "        a = np.einsum('ij,jk->ijk',np.eye(sftmx.shape[0]),sftmx)\n",
    "        b = np.einsum('ij,kj->ikj',sftmx,sftmx)\n",
    "        dHdZ = a - b\n",
    "        return dHdZ\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Loss functions or 'Cost functions' are what are to be minimised. They act as a proxy of the performance of the NN. \n",
    "\n",
    "For more comprehension: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "\n",
    "Up to now we discussed binary classification. Suppose we have three classes; the function one_hot generates a vector $y_0=[1,0,0]$ for class 0, $y_1=[0,1,0]$ for class 1 and $y_2=[0,0,1]$ for class 2 for each data point.\n",
    "\n",
    "Our network gives for each data point three probabilities: $p_0$ for that point to be type $0$, $p_1$ for $1$, $p_2$ for $2$. \n",
    "\n",
    "Next consider, again for any point point the quantity\n",
    "<ul>\n",
    "    $\\displaystyle - y_0 \\ln(p_0) - y_1 \\ln(p_1) - y_2 \\ln(p_2)$ \n",
    "</ul>\n",
    "Now assume that our data point is in class $1$ but has a small value for $p_1$. Then the only non-zero term $- y_1 \\ln(p_1)$ becomes large, just as we have seen before in binary classification.\n",
    "\n",
    "Then summing over all data points and dividing by the number of data points gives our cost function.\n",
    "\n",
    "Note 1: $H$ is the output of the network, which in this case is the probability distribution of the classes.\n",
    "Note 2: The differential of the Loss function we need is with respect to $Z$, so using the chain rule for partial derivatives: $\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial H} \\cdot \\frac{\\partial H}{\\partial Z}  $\n",
    "\n",
    "For a neat explanation of how to derive the CE_Loss derivative w.r.t $Z$ look at: https://deepnotes.io/softmax-crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CE_loss:\n",
    "    def get_loss(H,Y):\n",
    "        #L = Cross_Entropy_Loss(H,Y)\n",
    "        \n",
    "        # INSERT CODE HERE #\n",
    "        \n",
    "        return L\n",
    "    \n",
    "    def diff(H,Y):\n",
    "        #dZ = diff(Cross_Entropy_Loss(H,Y)) w.r.t Z \n",
    "        \n",
    "        # INSERT CODE HERE #\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The definition of the optimization problem.\n",
    "\n",
    "A neural net consists of layers of nodes. In its simplest form such a network can be described as follows.\n",
    "<ul>\n",
    "    <li>\n",
    "    One data point, $X$, is input for the first layer. \n",
    "    </li>\n",
    "    <li>\n",
    "    The output of layer $k-1$ is input for layer $k$. \n",
    "    </li>\n",
    "    <li>\n",
    "    The weight matrix $W(k)$ that linearly transforms output from layer $k-1$ is \n",
    "    of size $n_{k} \\times n_{k-1}$. Here $n_m$ is the number of nodes in layer $m$.\n",
    "    Moreover the bias of this layer, $b(k)$, is an $n_{k}$ vector.\n",
    "        \n",
    "    In formulas\n",
    "    <ul>\n",
    "        $Z = W(k) \\cdot A(k) + b(k)$\n",
    "    </ul>\n",
    "    followed by\n",
    "    <ul>\n",
    "        $A(k+1) = g_k(Z)$, with $g_k$ the activation of layer $k$.\n",
    "    </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        The output of the network is the data on the last layer. For a classification\n",
    "        problem the output of the current network (i.e., with these weights) gives the \n",
    "        probability of that data being in class $c$, $c=1, \\ldots, C$.\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "**The problem is: for which entries in matrix $W(k)$ and in vector $b(k)$, does the network \n",
    "work the best as possible.** \n",
    "\n",
    "Note that this can be (is) a huge optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "\n",
    "Initialization of the weights of a neural network can affect its performance. \n",
    "Empirically, it was found that each activation function has its own 'optimal' initialization\n",
    "\n",
    "Initializations taken from : https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Matrix\n",
    "def init_theta(n1,n2,activation):\n",
    "    #n1 = number of nodes in prev layer (input)\n",
    "    #n2 = number of nodes in next layer (output)\n",
    "    if activation in [sigmoid,softmax]:\n",
    "        M = np.random.randn(n2,n1)*np.sqrt(2./n1)\n",
    "    \n",
    "    elif activation in [relu] :\n",
    "        M = np.random.randn(n2,n1)*np.sqrt(1./n1)\n",
    "    \n",
    "    elif activation == tanh:\n",
    "        M = np.random.randn(n2,n1)*np.sqrt(1./(n1+n2))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Layer of neurons\n",
    "\n",
    "Recall that each layer has a weight matrix and a bias vector associated with it.\n",
    "During forward pass, the weights and bias are used in computing basis function Z of input vector A(l-1) and subsequently activations A(l) for the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self, n_prev, n_next, activation):\n",
    "        # Initialise the parameters related to the layer using init_theta(n1,n2,activation)\n",
    "        self.W = # INSERT CODE HERE #\n",
    "        self.B = # INSERT CODE HERE #\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        #ignore this for now\n",
    "        self.V_dW = np.zeros(self.W.shape)\n",
    "        self.V_dB = np.zeros(self.B.shape)\n",
    "        \n",
    "    def forward(self, A0):\n",
    "        \n",
    "        #Compute Z and A using self.W, self.B and self.activation.activate()\n",
    "        \n",
    "        self.Z = # INSERT CODE HERE #\n",
    "        self.A = # INSERT CODE HERE #\n",
    "        return self.A\n",
    "    \n",
    "    def grad(self, dZ, W, A0, m):\n",
    "        \n",
    "        # Apply Chain Rule to find dA, dAdZ, dZ, dW, dB  (take dA =~ deltaJ/deltaA, same for dZ, dW, dB etc)\n",
    "        \n",
    "        dA = # INSERT CODE HERE #\n",
    "        dAdZ = # INSERT CODE HERE #\n",
    "        self.dZ = # INSERT CODE HERE #\n",
    "        self.dW = # INSERT CODE HERE #\n",
    "        self.dB = # INSERT CODE HERE #\n",
    "        \n",
    "# we define out_grad separately because we find dH/dZ directly and not dH/dA*dA/dZ. If you have any ideas, feel free to change this!   \n",
    "    def out_grad(self, dZ, A0, m):\n",
    "        \n",
    "        self.dZ = dZ\n",
    "        #Compute self.dW and self.dB \n",
    "        self.dW = # INSERT CODE HERE #\n",
    "        self.dB = # INSERT CODE HERE # remember to pass keepdims=True as last parameter!\n",
    "        \n",
    "    def step(self, lr, beta):\n",
    "        #consider these instead of dW and dB. V_dW and V_dB are used to build 'momentum'. if beta is 0, this is effectively dW and dB\n",
    "        self.V_dW = (beta * self.V_dW + (1. - beta) * self.dW)\n",
    "        self.V_dB = (beta * self.V_dB + (1. - beta) * self.dB)\n",
    "        \n",
    "        #the update steps for the weight and bias use self.V_dW and self.V_dB intead of self.dW and self.dB\n",
    "        self.W = self.W # INSERT CODE HERE #\n",
    "        self.B = self.B # INSERT CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "NNs can be quite tricky to optimize because of the sheer number of parameters. Optimization can quickly get out of hand. A mini-batch gradient descent method is empirically shown to perform best and with decent stability. This is also called stochastic gradient descent interchangeably.\n",
    "\n",
    "The particular SGD we have defined uses SGD with momentum, which uses exponentailly weighted averages to add some 'momentum' to the gradient descent. There are 'better' modifications to SGD that can be done such as RMS_Prop, Adam etc. which are not included for sake of simplicity. This works well enough for relatively shallow networks. When Deep NNs are used, these more advanced optimizers become handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(batch_size,X,Y,model,lr,beta):\n",
    "    m = np.shape(X)[1]\n",
    "    \n",
    "    for i in range(0,m,batch_size):\n",
    "        \n",
    "        X_batch = X[:,i:i+batch_size]\n",
    "        Y_batch = Y[:,i:i+batch_size]\n",
    "        \n",
    "        #call f_pass of model and pass input data\n",
    "        #call back_prop\n",
    "        #call optim\n",
    "        \n",
    "        # INSERT CODE HERE #\n",
    "    \n",
    "    return model.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the optimization problem: training.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        Take a data point and go through the forward pass. \n",
    "    </li>\n",
    "    <li>\n",
    "        Compute the loss function. This is a function of the actual label $y$ and \n",
    "        predicted label $y_h$, $f(y, y_h)$. It captures how far off our predictions \n",
    "        are from the actual target.\n",
    "    </li>\n",
    "    <li>\n",
    "        Backward Propagation. In this step, we calculate the gradients of the loss \n",
    "        function $f(y, y_h)$ with respect to $A$, $W$, and $b$ called $dA$, $dW$ \n",
    "        and $db$. Using these gradients we update the values of the parameters from the \n",
    "        last layer to the first.\n",
    "    </li>\n",
    "</ul>\n",
    "Repeat these stepsfor $n$ iterations/epochs till we feel we have minimized the loss function, without overfitting the train data (more on this later!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The NN is finally trained for n_epochs and a plot of the training accuracy and test accuracy over n_epochs is displayed. This helps determine overfit/underfit and tuning of the hyperparameters.\n",
    "\n",
    "Nothing fancy to be done here. We are just calling SGD on our model: mnist_net using the hyperparameters we defined previously. We are appyling inv_one_hot transformations in order to be able to calculate the metric we defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, Y, X_test, Y_test, metric, n_epochs=100, batch_size=4, lr=0.01, lr_decay=1, beta=0, reg_lambda=0):\n",
    "    data_size = X.shape[1]\n",
    "    for e in range(n_epochs):\n",
    "        #shuffle dataset\n",
    "        # shuffling helps remove possible serial-relations between data, and reduces 'memorisation'\n",
    "        np.random.seed(138)\n",
    "        shuffle_index = np.random.permutation(data_size)\n",
    "        X, Y = X[:,shuffle_index], Y[:,shuffle_index]\n",
    "\n",
    "        #SGD with momentum\n",
    "        loss = SGD(batch_size,X,Y,model,lr,beta, reg_lambda)\n",
    "        \n",
    "        #decay helps decrease the size of steps over time, improving stability and convergence\n",
    "        lr = lr*lr_decay\n",
    "        \n",
    "        #train accuracy\n",
    "        H = model.f_pass(X)\n",
    "        tr_acc = metric(H,Y)\n",
    "        \n",
    "        #test accuracy\n",
    "        H = model.f_pass(X_test)\n",
    "        acc = metric(H,Y_test)\n",
    "\n",
    "        #plot train accuracy and test accuracy vs epochs\n",
    "        plt.plot(e,tr_acc, 'bo')\n",
    "        plt.plot(e,acc,'ro')\n",
    "        clear_output()\n",
    "        print(f\"epoch:{e+1}/{n_epochs} | Loss:{loss:.4f} | \\\n",
    "            Train Accuracy: {tr_acc:.4f} | Test_Accuracy:{acc:.4f}\")\n",
    "        \n",
    "    #plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Logistic Regression as a 2-Layer NN\n",
    "\n",
    "Let's repeat the logistic regression exercise on ex2data1.csv as done in week 2, this time based on a 2-layer NN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data, and partitioning train and test set\n",
    "\n",
    "If you want to find out what happens when the input is not normalized, comment out the normalize function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Train/test split seems to cause a high test error - Shuffle df for better results\n",
    "import pandas as pd\n",
    "from atlas_ml import *\n",
    "\n",
    "#change the path to the data per your directory structure\n",
    "#  INSERT CODE HERE: \n",
    "# df = pd.read_csv('Datasets/Regression/ex2data1.csv')\n",
    "\n",
    "\n",
    "A2 = df.values.T\n",
    "X1 = A2[0:2,0:-10]\n",
    "X2 = normalize(X1)\n",
    "Y1 = A2[2:3,0:-10]\n",
    "Y2 = one_hot(Y1,2)\n",
    "X1_test = A2[0:2,-10:]\n",
    "X2_test = normalize(X1_test)\n",
    "Y1_test = A2[2:3,-10:]\n",
    "Y2_test = one_hot(Y1_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(data, label_x, label_y, label_pos, label_neg, axes=None):\n",
    "    if axes == None:\n",
    "        axes = plt.gca()\n",
    "    neg = data[:,2] == 0\n",
    "    pos = data[:,2] == 1\n",
    "    axes.scatter(data[pos][:,0], data[pos][:,1], marker='+', c='k', s=60, linewidth=2, label=label_pos)\n",
    "    axes.scatter(data[neg][:,0], data[neg][:,1], c='y', s=60, label=label_neg)\n",
    "    axes.set_xlabel(label_x)\n",
    "    axes.set_ylabel(label_y)\n",
    "    axes.legend(frameon= True, fancybox = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we define the Logistic regression model as a 2 layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic:\n",
    "    def __init__(self, X_size, Y_size, lossfn):\n",
    "        self.regressor = layer(X_size, Y_size, softmax)\n",
    "        self.lossfn = lossfn\n",
    "        \n",
    "    def f_pass(self, X):\n",
    "        self.H = self.regressor.forward(X)\n",
    "        return self.H\n",
    "    \n",
    "    def back_prop(self, X, Y, batch_size, reg_lambda):\n",
    "        m = batch_size\n",
    "        self.loss = self.lossfn.get_loss(self.H,Y)\n",
    "        dZ = self.lossfn.diff(self.H,Y)\n",
    "        self.regressor.out_grad(dZ, X, m, reg_lambda)\n",
    "    \n",
    "    def optim(self, lr, beta=0):\n",
    "        self.regressor.step(lr,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = np.shape(X2)[0]\n",
    "n_out = np.shape(Y2)[0]\n",
    "log_reg = Logistic(n_in,n_out,CE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "lr = 0.01\n",
    "lr_decay = 1\n",
    "batch_size = X2.shape[1] # Batch gradient descent\n",
    "beta = 0\n",
    "\n",
    "#regularization parameter labmda\n",
    "reg_lamda = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call train() with appropriate model and parameters\n",
    "# INSERT CODE HERE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = log_reg.f_pass(X2)\n",
    "H2 = inv_one_hot(H)\n",
    "A3 = np.vstack([X1,H2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(A3.T, 'Exam 1 score', 'Exam 2 score', 'Admitted', 'Not admitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotData(A2.T, 'Exam 1 score', 'Exam 2 score', 'Admitted', 'Not admitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: 1 Hidden Layer \n",
    "\n",
    "Now we add a hidden layer to this Logistic-Regression-like NN and try to get similar or better results.\n",
    "Make note of how the class HL_net is structured and try to figure out what each statement means. This will be needed for Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HL_net:\n",
    "    def __init__(self, X_size, Y_size, lossfn):\n",
    "        self.L1 = layer(X_size, 100, relu)\n",
    "        self.L2 = layer(100, 20, relu)        \n",
    "        self.L3 = layer(20, Y_size, softmax)\n",
    "        self.lossfn = lossfn\n",
    "        \n",
    "    def f_pass(self, X):\n",
    "        A1 = self.L1.forward(X)\n",
    "        A2 = self.L2.forward(A1)\n",
    "        A3 = self.L3.forward(A2)\n",
    "        self.H = A3\n",
    "        return self.H\n",
    "    \n",
    "    def back_prop(self,X,Y, batch_size,reg_lambda):\n",
    "        m = batch_size\n",
    "        self.loss = self.lossfn.get_loss(self.H,Y)\n",
    "        dZ = self.lossfn.diff(self.H,Y)\n",
    "        self.L3.out_grad(dZ, self.L2.A, m,reg_lambda)\n",
    "        self.L2.grad(self.L3.dZ, self.L3.W, self.L1.A, m, reg_lambda)\n",
    "        self.L1.grad(self.L2.dZ, self.L2.W, X, m, reg_lambda)\n",
    "    \n",
    "    def optim(self, lr, beta=0):\n",
    "        self.L1.step(lr,beta)\n",
    "        self.L2.step(lr,beta)\n",
    "        self.L3.step(lr,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = np.shape(X2)[0]\n",
    "n_out = np.shape(Y2)[0]\n",
    "bin_clas = HL_net(n_in,n_out,CE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "lr = 0.01\n",
    "lr_decay = 0.99\n",
    "batch_size = 4 # mini-batch gradient descent here\n",
    "beta = 0.9\n",
    "\n",
    "#regularization parameter labmda\n",
    "reg_lamda = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#call train() with appropriate model and parameters\n",
    "# INSERT CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how training accuracy is way higher than test accuracy?\n",
    "\n",
    "This is mainly because of overfitting, and partially because we have such a small dataset over which we repeat training.\n",
    "\n",
    "The act of stopping training at a relatively high training accuracy such that the test accuracy does not decrease further w.r.t the train accuracy is called early stopping and this controls overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Let's Classify images!\n",
    "\n",
    "We finally get to define the NN below. We are free to choose the number of hidden layers and the respective sizes, the activation functions for each of these, the loss function etc. This is where the object oriented setup helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "    \n",
    "X = read_idx('data/MNIST/train/train-images-idx3-ubyte')   # Change path to data as required\n",
    "X = flatten_imgs(X)\n",
    "X = normalize(X)               # Try without this\n",
    "X = np.transpose(X)\n",
    "\n",
    "Y = read_idx('data/MNIST/train/train-labels-idx1-ubyte') # Change path to data as required\n",
    "Y = np.expand_dims(Y, axis=1)\n",
    "Y = np.transpose(Y)\n",
    "Y = one_hot(Y,10)\n",
    "\n",
    "X_test = read_idx('data/MNIST/test/t10k-images.idx3-ubyte') # Change path to data as required\n",
    "X_test = flatten_imgs(X_test)\n",
    "X_test = normalize(X_test)     # Try without this\n",
    "X_test = np.transpose(X_test)\n",
    "\n",
    "Y_test = read_idx('data/MNIST/test/t10k-labels.idx1-ubyte') # Change path to data as required\n",
    "Y_test = np.expand_dims(Y_test, axis=1)\n",
    "Y_test = np.transpose(Y_test)\n",
    "Y_test = one_hot(Y_test,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualise what one sample of our dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "plt.imshow(X[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(Y[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_net:\n",
    "    def __init__(self, X_size, Y_size, lossfn):\n",
    "        \n",
    "        # Feel free to change the number of nodes within the layers!\n",
    "        # To add more layers, all you have to do is add them here,make appropriate additions in f_pass, back_prop and optim as well!\n",
    "        self.L1 = layer(X_size, 500, relu)\n",
    "        self.L2 = layer(500, 150, relu)\n",
    "        self.L3 = layer(150, Y_size, softmax)\n",
    "        \n",
    "        self.lossfn = lossfn\n",
    "        \n",
    "    def f_pass(self, X):\n",
    "        \n",
    "        # Compute output 'self.H' using the self.L'#'.forward() functions for each layer\n",
    "        \n",
    "        # INSERT CODE HERE #\n",
    "        \n",
    "        return self.H\n",
    "    \n",
    "    def back_prop(self,X,Y, batch_size):\n",
    "        #this will be passed to functions you call below\n",
    "        m = batch_size\n",
    "        \n",
    "        #this is just for bookkeeping, not used in backprop\n",
    "        self.loss = self.lossfn.get_loss(self.H,Y)   \n",
    "        \n",
    "        dZ = # INSERT CODE HERE #\n",
    "        \n",
    "        #call out_grad for the last layer\n",
    "        #call grad for each layer\n",
    "        \n",
    "        # INSERT CODE HERE #\n",
    "\n",
    "        \n",
    "    def optim(self, lr, beta=0):\n",
    "        # use self.L'#'.step() for each layer to step their weights\n",
    "        \n",
    "        # INSERT CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "n_in = np.shape(X)[0]\n",
    "n_out = np.shape(Y)[0]\n",
    "mnist_net = MNIST_net(n_in,n_out,CE_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Parameters that define the training and the NN are called Hyperparameters ( so Meta!)\n",
    "These include the learning rate, the batch size for the SGD, the decay in the learning rate etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters feel free to tweak these!\n",
    "\n",
    "#lies in [1,X.shape[1]], preferably powers of 2\n",
    "# 1 -> stochastic descent\n",
    "# X.shape[1] -> batch descent\n",
    "batch_size = 32\n",
    "\n",
    "#learning rate\n",
    "lr = 0.075\n",
    "\n",
    "#number of iterations\n",
    "n_epochs = 10\n",
    "\n",
    "#learning rate decay parameter, lies in [0,1]\n",
    "lr_decay = 0.9\n",
    "\n",
    "#momentum parameter, lies in [0,1]\n",
    "beta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call train() with appropriate model and parameters\n",
    "# INSERT CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: The empirical process of practical deep learning\n",
    "\n",
    "We have defined a 3-layer NN to classify handwritten digits.\n",
    "\n",
    "Play around with the depth of the NN (add more layers), the width of the layers (add more nodes), the hyperparameters, the activation functions and observe the effects these bear on the performance of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = np.shape(X2)[0]\n",
    "n_out = np.shape(Y2)[0]\n",
    "bin_clas = HL_net(n_in,n_out,CE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "lr = 0.01\n",
    "lr_decay = 0.99\n",
    "beta = 0.9\n",
    "\n",
    "#regularization parameter labmda\n",
    "reg_lamda = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call train() with appropriate model and parameters\n",
    "# INSERT CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your observations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onlinepub",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
