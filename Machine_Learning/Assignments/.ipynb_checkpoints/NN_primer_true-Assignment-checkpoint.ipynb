{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 (2 Weeks): Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updates:\n",
    "\n",
    "1. Added a 'Layout of the Assignment' text block outlining the Assignment and motivating its structure\n",
    "2. One hot encoding and inverse functions are now provided. It is recommended that you attempt to recreate these functions in order to understand what they're doing.\n",
    "3. Unit test code block and expected output are provided after each major code block\n",
    "4. The class Layer is clearly defined and motivated\n",
    "5. Class functions now have self passed in all definitions. object creations are done for you in order to avoid issues that could arise out of improper creations.\n",
    "6. Structure of Model class is provided prior to SGD since SGD requires calling of member functions of this class\n",
    "7. Variable names referring to partial derivatives are more explicit and mention which layer they correspond to. For example dZ, A0, etc are now dL_dZ_next respectively.\n",
    "8. Removed all instances of reg_lambda, and beta. So SGD is now without momentum and weight updates are not regularized.\n",
    "9. #INSERT CODE HERE# is now ''' INSERT CODE HERE ''' - highlighted in red for more visibility\n",
    "10. In some cases, example code is provided. This will work once uncommented. It is recommended to understand what this code is doing and what it means conceptually.\n",
    "\n",
    "##### Remember to change paths of data files and image files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks (NN) are essentially the closest we've come to universal function approximators. All the hype you've been hearing about Deep Learning and \"AI\" are due to advances in NNs. These beasts deserve a separate course, but here you'll get to learn how a basic NN works\n",
    "\n",
    "If you recall logistic regression, we model a classification problem as a non-linear function of the input vector.\n",
    "When the problem in question is quite complicated, and the number of possible input features are large, simple classification, regression or classic ML techniques have limited ability. Composing functions of functions, say $f( g( h(X,W_{0}),W_{1}),W_{2}) $ would be better equipped to 'learn' more complicated functions. \n",
    "\n",
    "NNs are effectively compositions of functions consisting of layers of 'nodes'. The permuted connections between these nodes allow complex 'functions' to be 'learnt'. Depicted below is a 3-layer NN, consisting of an input layer, a 'hidden' layer and an output layer.\n",
    "\n",
    "Why provide an elaborate explanation when this beautiful visualization exists:\n",
    "https://www.youtube.com/watch?v=aircAruvnKk ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To start off, let's first visualise logistic regression in terms of a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"Assignments/nnlogic.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a 2-layer NN, which is effectively a logistic regression model. The input layer is X and output layer is a single node y. The connections from the input layer to the output layer represent the weights that are to be 'learnt', and represented as a matrix W such that sigmoid(W*X) = y\n",
    "\n",
    "So a 3-Layer NN with a h-neuron hidden layer would essentially be h ~logistic regressions of the input times y ~logistic regressions of the hidden layer. So there would be x*h + h*w weights to be learnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "The computation of the output layer by composing functions through each hidden layer is called forwards propagation. These functions are called 'activations' and are non-linear in nature. Let's say we have an input feature vector X, a single hidden layer witha sigmoid activation and an output layer with a sigmoid activation.\n",
    "\n",
    "A forward-pass is then given by:\n",
    "\n",
    "$ Y' = \\sigma( W_{2} \\cdot  ( \\sigma( W_{1} \\cdot X)))$\n",
    "\n",
    "Where $W_{1}$ and $W_{2}$ are the weights/parameters to be learnt, $X$ is the input vector and $Y'$ is the output vector of the NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Propagation\n",
    "\n",
    "Remember how we optimised the model in logistic regression? We used the partial differentials of the Cost function w.r.t the weight we were trying to optimize. Same concept, but because we have a composition of functions, we use the chain rule. So let's say cost function $J(x,W_{1},W_{2}, Y) = f( g(x,W_{1}),W_{2},Y) $. \n",
    "Then $  \\frac{\\partial J}{\\partial W_{1}} = \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial W_{1}} $\n",
    "\n",
    "The optimising step would then be $ W_{1} \\rightarrow W_{1} - \\alpha \\cdot \\frac{\\partial J}{\\partial W_{1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to get a better grasp of how NNs work, let's build one from scratch using numpy and python\n",
    "\n",
    "This tutorial uses python classes in order to build a kind of NN-library. This is to help us breakdown the different components that go into a NN and then be able to build different architectures of NN more easily. \n",
    "\n",
    "This also allows us to peek into the inner workings of the NN since we can print the weights between any two layers to understand what is happening there. \n",
    "\n",
    "You are required to complete the code where it says ''' INSERT CODE HERE '''\n",
    "\n",
    "It is advised to run 'unit' tests on each block you finish, where possible, by perhaps using a random array.\n",
    "\n",
    "##### Note: The dimensioning of data is as follows: nxm, n =number of features, m = number of examples. This is the transposed version of what was being used for the previous assignments.\n",
    "\n",
    "#### Some of the links also provide code. This code is usually not compatible with how our code is structured. It is recommended to just use these links for the derivation, formulas and concepts.\n",
    "\n",
    "### A quick primer on python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object:                    #This is a class, it can have 'members' that are functions or data\n",
    "    def __init__(self,x):          # the __init__ function initialises the class Object whenever a object is declared\n",
    "        self.A = x               # here, a data member A (accessed as self.A) is assigned x whenever an object is created\n",
    "    def mem_function(self,X):    # this is a member function, it can access the class's data and use external parameters\n",
    "        self.B = self.A+X        # here another data member is created and assigned a value\n",
    "        \n",
    "obj = Object(2)                  # here an Object object is created as obj. Here, Object(2) is \n",
    "                                 # basically passing 2 to the __init__ function of the object\n",
    "print(\"object\", obj)\n",
    "print(\"it's a Value\", obj.A)\n",
    "obj.mem_function(3)              # here the member function is called using <class_name>.<functoin_name>, \n",
    "                                 # self is not passed as a parameter, the object does this implicitly.\n",
    "obj.B                            # this is how the object's data can be accessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layout of the Assignment\n",
    "\n",
    "We will be building the NN(s) using objects in order to make Part C and Part D quicker and simpler.\n",
    "The NN(s) we will build are classifiers. We have two main datasets:\n",
    "1. a binary classification problem (same as in Logistic Regression assignment)\n",
    "2. a handwritten digit classification problem called MNIST\n",
    "\n",
    "In order to be able to predict the class, we will use one-hot representation of the target data. This allows the NN to output a probability distribution of which class the particular datapoint might belong to.\n",
    "\n",
    "The functionality of the NN(s) we will build is as follows:\n",
    "\n",
    "We have an input dataset, $X$, of shape $(n,m)$, With $n=$ number of features and $m =$ number of examples or batch-size.\n",
    "\n",
    "The NN outputs a probability distribution, $H$, with shape $(n_{class},m)$ of the classes to which each example/datapoint belongs.\n",
    "\n",
    "We have a fixed number of 'layers' between $X$ and $H$, with each $l^{th}$ 'layer' having weights $W_{l}$, and a bias $B_{l}$.\n",
    "\n",
    "Each $l^{th}$ 'layer' has an input vector $A_{l-1}$, a linear transformation : ($W \\cdot A_{l-1} + B_{l}$) of A0 resulting in vector $Z_{l}$ and an activation function activation($Z_{l}$) which activates on $Z_{l}$ to give a vector $A_{l}$\n",
    "\n",
    "For the first layer, $A_{l-1}$ = X, and for the last layer $A_{l}$ = $H$, the output vector or 'hypothesis'\n",
    "\n",
    "#### Note: The output of the NN will be a probability distribution when Softmax is used as output activation. \n",
    "\n",
    "In order to prevent code repetition, we define a class Layer and this will allow us to stack several layers with only a few lines.\n",
    "\n",
    "\n",
    "We first define helper functions that will help us load and process the dataset. \n",
    "\n",
    "Next, we define a metric to assess the performance of the models we will use later.\n",
    "\n",
    "We then define activation functions as classes, with each activation function having an activate() and diff() member fucntion. This way, when we need to find sigmoid(Z), we call sigmoid.activate(Z) and when we need to find $\\frac{\\partial Sigmoid(Z)}{\\partial Z}$, we call sigmoid.diff(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The below functions are trivial and can be skipped, unless, you want to come up with a cleaner way to do them ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "    \n",
    "def flatten_imgs(X):\n",
    "    n_img = np.shape(X)[0]\n",
    "    h = np.shape(X)[1]\n",
    "    w = np.shape(X)[2]\n",
    "    size_arr = h*w\n",
    "    return np.reshape(X,(n_img,size_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding \n",
    "\n",
    "One hot encoding is basically representing a class as a boolean vector: e.g if we have 3 classes (let's say classes 1,2,3 ) and we want to represent class '1'. We would do this as [1 0 0]$^{T}$. Class '2' would be [0 1 0]$^{T}$ and so on.. \n",
    "\n",
    "One hot encoding allows us to output a probability distribution vector [p1 p2 p3]$^{T}$ such that these probabilities represent the likelihood of the particular class. So a vector [0.9 0.02 0.08]$^{T}$ would mean that the output represents class 1 with a 0.9 probability\n",
    "\n",
    "one_hot() should convert a 1xm numpy array to an nxm numpy array, where n is the number of class (n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y,n_class):\n",
    "    #accept a (1xm) 'label' numpy-array, number of classes  \n",
    "    #and return a nxm 'one-hot' numpy-array, where n=number of classes \n",
    "    ''' INSERT CODE HERE '''\n",
    "    \n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "Y = np.array([[0,1,2,3,4]])\n",
    "n_class = 5\n",
    "O = one_hot(Y,n_class)\n",
    "print(O)\n",
    "# Should output a matrix equivalent to a 5x5 identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to have the inverse encoding from one-hot representation. This is the reverse of the one_hot function. So an array of one_hot encoded vectors is passed as parameter and the corresponding label array is returned. \n",
    "\n",
    "#### This can be used to convert our output probability distributions of a batch of data points to the corresponding label array. One way to do this would be to find the index of the maximum element of each vector and use that as the label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_one_hot(O):\n",
    "    n_class = np.shape(O)[0]\n",
    "    length = np.shape(O)[1]\n",
    "    \n",
    "    Y = np.zeros((1,length))\n",
    "    \n",
    "    for i in range(length):\n",
    "        j = np.argmax(O[:,i])\n",
    "        Y[0,i] = j\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unit test\n",
    "\n",
    "inv_one_hot(O)\n",
    "\n",
    "#Should output array array([[0., 1., 2., 3., 4.]]) if O is 5x5 identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Neural networks can quickly get messy if the input data is not normalized. You could try implementing the NN without normalizing the data and find out how it affects the performance.\n",
    "\n",
    "There is one common way of normalizing data, namely by computing the mean $\\mu$ and standard devitation $\\sigma$\n",
    "<ul> \n",
    "    $\\displaystyle\\mu = \\frac{1}{M}\\sum_{m=0}^{M-1} X_m$,  $\\displaystyle ~~~~\\sigma = \\sqrt{\\frac{1}{M}\\sum_{m=0}^{M-1} (X_m -\\mu)^2}$\n",
    "</ul>\n",
    "and then defining \n",
    "<ul> \n",
    "    $\\displaystyle Y_m = \\frac{X_m - \\mu}{\\sigma}$  $~~~~ m = 0, \\ldots, M-1$\n",
    "</ul>\n",
    "In this case the new data points $Y_m$ are centered around value $0$ with a spread $1$.\n",
    "\n",
    "Here M is the number of examples i.e the number of columns in X. \n",
    "So mean = vector of mean of each row\n",
    "std = vector of std of each row\n",
    "\n",
    "If using numpy mean and std functions, specify the axis and remember to pass keepdims=True to maintain the shape of the mean and std vectors at nx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    mean = np.mean(X,axis=1,keepdims=True)\n",
    "    std =  np.std(X,axis=1,keepdims=True)\n",
    "    N_X = (X-mean)/(std)\n",
    "    \n",
    "    return N_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "In order to effectively measure the performance of our model we can define a 'metric'. Here we use a straight-forward 'right or not' method. This returns a [0,1] value indicating the accuracy of the model. This will later be passed as 'metric' in the train() function, so remember to replace metric with model_accuracy when calling train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(H,Y):\n",
    "    # Y has to be one-hot matrix\n",
    "    # H can be either one hot or softmax output.\n",
    "    n = np.shape(H)[1]\n",
    "    err = 0\n",
    "    O = inv_one_hot(H)\n",
    "    L = inv_one_hot(Y)\n",
    "    for i in range(n):\n",
    "        if O[0,i]!=L[0,i]:\n",
    "            err += 1\n",
    "    accuracy = (1 - err/n)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Each node except those in the input layer represents an activation function, which applies a 'non-linearity' to its input parameter.\n",
    "\n",
    "ML Researchers have empirically found several different activation functions to work with their respective pros and cons. Listed below are the most popular.\n",
    "Softmax is a unique activation function in that it is used in multiclass classification i.e for categorical data.\n",
    "\n",
    "For some more comprehension: https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Activation|Equation|\n",
    "|----|---------|\n",
    "|<img width=50/>|<img width=100/>|\n",
    "|Sigmoid| $ \\sigma(x) = \\frac{1}{1+e^{-x}} $ |\n",
    "| Tanh | $tanh(x)$ |\n",
    "| ReLu | $ max(0,x)$    |\n",
    "| Softmax| $ \\frac{e^{x_{i}}}{\\sum e^{x_{i}}} $ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    def activate(self,Z):\n",
    "        #compute A = sigmoid(Z), Z is an nxm numpy array, A should also be an nxm numpy array\n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def diff(self,Z):\n",
    "        #compute dA_dZ = differential of sigmoid w.r.t Z \n",
    "        \n",
    "        #code provided below, just try to understand what dA_dZ is conceptually\n",
    "        #dA_dZ = np.multiply(self.activate(Z),(1-self.activate(Z)))\n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        return dA_dZ\n",
    "    \n",
    "class relu:\n",
    "    def activate(self,Z):\n",
    "        #compute A = relu(Z), Z is an nxm numpy array, A should also be an nxm numpy array\n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def diff(self,Z):\n",
    "        #compute dA_dZ = differential of relu w.r.t Z \n",
    "        \n",
    "        #code provided below, just try to understand what dA_dZ is conceptually\n",
    "        #dA_dZ = 1*(Z>0)\n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        return dA_dZ\n",
    "\n",
    "class tanh:\n",
    "    def activate(self,Z):\n",
    "        #compute A = tanh(Z), Z is an nxm numpy array, A should also be an nxm numpy array\n",
    "        #numpy has an inbuilt tanh function, you can just call that\n",
    "        \n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        return A\n",
    "\n",
    "    def diff(self,Z):\n",
    "        #compute dA_dZ = differential of tanh w.r.t Z \n",
    "        \n",
    "        #code provided below, just try to understand what dA_dZ is conceptually\n",
    "        #dA_dZ = 1 - (np.multiply(self.activate(Z),self.activate(Z)))\n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        return dA_dZ\n",
    "\n",
    "    \n",
    "# Let this be\n",
    "class softmax:\n",
    "    def activate(self,Z):\n",
    "        \n",
    "        e_Z = np.exp(Z- np.max(Z,axis=0))\n",
    "        A = e_Z / e_Z.sum(axis=0)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "# Just for formality. Not recommended to be used. \n",
    "# w.r.t Z is almost never needed since we compute C.E loss derivative w.r.t Z directly viz  H-Y\n",
    "    def diff(self,Z):\n",
    "        sftmx = self.activate(Z)\n",
    "        a = np.einsum('ij,jk->ijk',np.eye(sftmx.shape[0]),sftmx)\n",
    "        b = np.einsum('ij,kj->ikj',sftmx,sftmx)\n",
    "        dH_dZ = a - b\n",
    "        return dH_dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unit test\n",
    "# You can try sigmoid(), relu(),tanh() etc here.\n",
    "activation = tanh()\n",
    "\n",
    "Z = np.array([[-1,2,3]]).T\n",
    "\n",
    "print(\"activation(Z)=\\n\",activation.activate(Z))\n",
    "#should return a 3x1 array of corresponding activation values.\n",
    "print(\"\\ndA/dZ = \\n\",activation.diff(Z))\n",
    "#should return a 3x1 array of corresponding differential values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Loss functions or 'Cost functions' are what are to be minimised. They act as a proxy of the performance of the NN. \n",
    "\n",
    "For more comprehension: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "\n",
    "Up to now we discussed binary classification. Suppose we have three classes; the function one_hot generates a vector $y_0=[1,0,0]$ for class 0, $y_1=[0,1,0]$ for class 1 and $y_2=[0,0,1]$ for class 2 for each data point.\n",
    "\n",
    "Our network gives for each data point three probabilities: $p_0$ for that point to be type $0$, $p_1$ for $1$, $p_2$ for $2$. \n",
    "\n",
    "Next consider, again for any point point the quantity\n",
    "<ul>\n",
    "    $\\displaystyle - y_0 \\ln(p_0) - y_1 \\ln(p_1) - y_2 \\ln(p_2)$ \n",
    "</ul>\n",
    "Now assume that our data point is in class $1$ but has a small value for $p_1$. Then the only non-zero term $- y_1 \\ln(p_1)$ becomes large, just as we have seen before in binary classification.\n",
    "\n",
    "Then summing over all data points and dividing by the number of data points gives our cost function.\n",
    "\n",
    "Note 1: $H$ is the output of the network, which in this case is the probability distribution of the classes.\n",
    "Note 2: The differential of the Loss function we need is with respect to $Z$, so using the chain rule for partial derivatives: $\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial H} \\cdot \\frac{\\partial H}{\\partial Z}  $\n",
    "\n",
    "For a neat explanation of how to derive the CE_Loss derivative w.r.t $Z$ look at: https://deepnotes.io/softmax-crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CE_loss:\n",
    "    def get_loss(self,H,Y):\n",
    "        # H,Y are both nxm numpy arrays. L is a single value that represents the loss of the batch (size m)\n",
    "        ''' INSERT CODE HERE '''        \n",
    "        #Compute L = Cross_Entropy_Loss(H, Y)\n",
    "\n",
    "        return L\n",
    "    \n",
    "    def diff(self,H,Y):\n",
    "        #Compute dL_dZ = diff(Cross_Entropy_Loss(H,Y) w.r.t Z)        \n",
    "        \n",
    "        n = Y.shape[0]\n",
    "        #code provided below, just try to understand what dA_dZ is conceptually\n",
    "        #dL_dZ = 1/n*(H-Y) \n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        return dL_dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unit test\n",
    "H = np.array([[0.1,0.1,0.8]]).T\n",
    "Y = np.array([[0,0,1]]).T\n",
    "loss = CE_loss()\n",
    "\n",
    "print(\"loss=\",loss.get_loss(H,Y))\n",
    "print(\"dL/dZ=\",loss.diff(H,Y))\n",
    "\n",
    "#should give one value for loss, and a nx1 array of corresponding differentials for diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The definition of the optimization problem.\n",
    "\n",
    "A neural net consists of layers of nodes. In its simplest form such a network can be described as follows.\n",
    "<ul>\n",
    "    <li>\n",
    "    One data point, $X$, is input for the first layer. \n",
    "    </li>\n",
    "    <li>\n",
    "    The output of layer $k-1$ is input for layer $k$. \n",
    "    </li>\n",
    "    <li>\n",
    "    The weight matrix $W_{k}$ that linearly transforms output from layer $k-1$ is \n",
    "    of size $n_{k} \\times n_{k-1}$. Here $n_m$ is the number of nodes in layer $m$.\n",
    "    Moreover the bias of this layer, $b(k)$, is an $n_{k}$ vector.    \n",
    "    In formulas\n",
    "    </li>\n",
    "    <ul>\n",
    "        $Z = W_{k} \\cdot A_{k} + B_{k}$\n",
    "    </ul>\n",
    "    followed by\n",
    "    <ul        $A_{k+1} = g_{k}(Z)$, with $g_{k}$ the activation of layer $k$.\n",
    "    </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        The output of the network is the data on the last layer. For a classification\n",
    "        problem the output of the current network (i.e., with these weights) gives the \n",
    "        probability of that data being in class $c$, $c=1, \\ldots, C$.\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "**The problem is: for which entries in matrix $W(k)$ and in vector $b(k)$, does the network \n",
    "work the best as possible.** \n",
    "\n",
    "Note that this can be (is) a huge optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "\n",
    "Initialization of the weights of a neural network can affect its performance. \n",
    "Empirically, it was found that each activation function has its own 'optimal' initialization\n",
    "\n",
    "Initializations taken from : https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta(n1,n2,activation):\n",
    "    #n1 = number of nodes in prev layer (input)\n",
    "    #n2 = number of nodes in next layer (output)\n",
    "    #activation is the class of activation, this is used to select the initialization method. \n",
    "    if activation in [sigmoid,softmax]:\n",
    "        M = np.random.randn(n2,n1)*np.sqrt(2./n1)\n",
    "    elif activation in [relu] :\n",
    "        M = np.random.randn(n2,n1)*np.sqrt(1./n1)\n",
    "    elif activation == tanh:\n",
    "        M = np.random.randn(n2,n1)*np.sqrt(1./(n1+n2))\n",
    "    else:\n",
    "        M = np.random.randn(n2,n1)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model/ NN will be a class. An example 3 layer NN is as follows:\n",
    "\n",
    "    class NN_model:\n",
    "        \n",
    "        def __init__(self, X_size, Y_size, lossfn):\n",
    "            self.L1 = layer(X_size, num_nodes1, relu)\n",
    "            self.L2 = layer(num_nodes1, num_nodes2, relu)        \n",
    "            self.L3 = layer(num_nodes2, Y_size, softmax)\n",
    "            self.lossfn = lossfn()\n",
    "        \n",
    "        def f_pass(self, X):\n",
    "            A1 = self.L1.forward(X)\n",
    "            A2 = self.L2.forward(A1)\n",
    "            A3 = self.L3.forward(A2)\n",
    "            self.H = A3\n",
    "            return self.H\n",
    "\n",
    "        def back_prop(self,X,Y, batch_size):\n",
    "            m = batch_size\n",
    "            \n",
    "            self.loss = self.lossfn.get_loss(self.H,Y)\n",
    "            dL_dZ = self.lossfn.diff(self.H,Y)\n",
    "            \n",
    "            self.L3.out_grad(dL_dZ, self.L2.A, m)\n",
    "            self.L2.grad(self.L3.dZ, self.L3.W, self.L1.A, m)\n",
    "            self.L1.grad(self.L2.dZ, self.L2.W, X, m)\n",
    "\n",
    "        def optim(self, lr):\n",
    "            self.L1.step(lr)\n",
    "            self.L2.step(lr)\n",
    "            self.L3.step(lr)\n",
    "            \n",
    "The f_pass() function does one forward pass through the network and outputs the Hypothesis vector $H$\n",
    "\n",
    "The back_prop() function performs back propagation through the network and stores the gradients of $W$ and $B$ for each layer.\n",
    "\n",
    "optim() updates the $W$ and $B$ of each layer.\n",
    "\n",
    "This is where the layer class is used. Each layer of the NN is an object of the class Layer, which we will describe next, and therefore, we can easily stack a bunch of layers with a few lines of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Layer of neurons\n",
    "\n",
    "Recall that each layer has a weight matrix and a bias vector associated with it.\n",
    "During forward pass, the weights and bias are used in computing $Z$ of input vector $A_{l-1}$ and subsequently activations $A_{l}$ for the layer\n",
    "\n",
    "A_prev means $A_{l-1}$; was $A0$ in the previous version of the assigment\n",
    "dL_dZ_next means $\\frac{ \\partial L}{\\partial Z_{l+1}}$ ;  was dZ in function parameter in the previous version of the assignment\n",
    "\n",
    "We design the layer class as follows:\n",
    "\n",
    "Layer has a forward() function that computes $Z_{l} = W \\cdot A_{l-1} + B$ and then $A_{l}(Z_{l}) = activation(Z_{l})$\n",
    "\n",
    "grad() is used to compute the gradient of $W_{l}$, $dW = \\frac {\\partial L}{\\partial W_{l}} $  and that of $B_{l}, dB = \\frac {\\partial L}{\\partial B_{l}} $ given $\\frac {\\partial L}{\\partial Z_{l+1}}$\n",
    "\n",
    "out_grad() is used to compute the gradient of $W_{l}$, $dW = \\frac {\\partial L}{\\partial W_{l}} $  and that of $B_{l}, dB = \\frac {\\partial L}{\\partial B_{l}} $ as well, but only when the layer is the output layer with the final activation. So, dZ in this case is the differential of the loss function w.r.t $Z_{l}$ ; so out_grad is given $ dZ =  \\frac {\\partial L}{\\partial Z_{l}} $ as parameter\n",
    "\n",
    "The network with k layers would look like this.\n",
    "\n",
    "$X$ -> [$Z_{1}(X,W_{1},B_{1})$ -> $A_{1}(Z_{1})$ ] -> [$Z_{2}(A_{1},W_{2},B_{2})$ -> $A_{2}(Z_{2})$] -> ... -> [$Z_{k}(A_{k-1},W_{k},B_{k})$ ->$ A_{k}(Z_{k})$] -> $L(A_{k},Y)$\n",
    "\n",
    "Where $L()$ is the loss function and Y is the label or target vector\n",
    "\n",
    "The whole backpropagation of the network, starting from the last $k^{th}$ layer would be as such:\n",
    "\n",
    "$dW_{k} = \\frac {\\partial L}{\\partial A_{k}} \\cdot \\frac {\\partial A_{k}}{\\partial Z_{k}} \\cdot \\frac {\\partial Z_{k}}{\\partial W_{k}} $\n",
    "\n",
    "$dB_{k} = \\frac {\\partial L}{\\partial A_{k}} \\cdot \\frac {\\partial A_{k}}{\\partial Z_{k}} \\cdot \\frac {\\partial Z_{k}}{\\partial B_{k}} $\n",
    "\n",
    "$dW_{k-1} = \\frac {\\partial L}{\\partial A_{k}} \\cdot \\frac {\\partial A_{k}}{\\partial Z_{k}} \\cdot \\frac {\\partial Z_{k}}{\\partial A_{k-1}}  \\cdot \\frac {\\partial A_{k-1}}{\\partial Z_{k-1}} \\cdot \\frac {\\partial Z_{k-1}}{\\partial W_{k-1}} $\n",
    "\n",
    "$dB_{k-1} =  \\frac {\\partial L}{\\partial A_{k}} \\cdot \\frac {\\partial A_{k}}{\\partial Z_{k}} \\cdot \\frac {\\partial Z_{k}}{\\partial A_{k-1}} \\cdot \\frac {\\partial A_{k-1}}{\\partial Z_{k-1}} \\cdot \\frac {\\partial Z_{k-1}}{\\partial B_{k-1}}$\n",
    "\n",
    "and so on. \n",
    "\n",
    "\n",
    "This can be made modular so we take one layer, let's say the $l^{th}$ layer , that looks like:\n",
    "\n",
    "$A_{l-1}$ -> [$ Z_{l}(A_{l-1},W_{l},B_{l})$ -> $A_{l}(Z_{l})$ ] -> $Z_{l+1} $\n",
    "\n",
    "Therefore for this layer's part in backpropagation,we have:\n",
    "\n",
    "#### If the layer is not the last layer:\n",
    "\n",
    "$dA_{l} = \\frac {\\partial L}{\\partial A_{l}} = \\frac {\\partial L}{\\partial Z_{l+1}} \\cdot W_{l} $\n",
    "\n",
    "This is because there is at least one layer next to the current layer with $\\frac {\\partial L}{\\partial Z_{l+1}}$\n",
    "\n",
    "$dZ_{l} = \\frac {\\partial L}{\\partial A_{l}} \\cdot \\frac {\\partial A_{l}}{\\partial Z_{l}} $\n",
    "\n",
    "$dW_{l} = \\frac {\\partial L}{\\partial A_{l}} \\cdot \\frac {\\partial A_{l}}{\\partial Z_{l}} \\cdot \\frac {\\partial Z_{l}}{\\partial W_{l}} $\n",
    "\n",
    "$dB_{l} = \\frac {\\partial L}{\\partial A_{l}} \\cdot \\frac {\\partial A_{l}}{\\partial Z_{l}} \\cdot \\frac {\\partial Z_{l}}{\\partial B_{l}}$\n",
    "\n",
    "These will be computed in grad()\n",
    "\n",
    "#### If the layer is the last layer $(k^{th})$:\n",
    "\n",
    "$dA_{k} = \\frac {\\partial L}{\\partial A_{k}} $\n",
    "$dZ_{k} = \\frac {\\partial L}{\\partial A_{k}} \\cdot \\frac {\\partial A_{k}}{\\partial Z_{k}} $\n",
    "\n",
    "In the case of using softmax as the output activation and CE_loss as the loss function, $dZ_{k} = \\frac{1}{n}(A_{k} - Y) = \\frac{1}{n}(H - Y)$, where $A_k = H$ as in hypothesis, and n is the number of classes. It is recommended for you to derive this result in order to be able to understand it.\n",
    "\n",
    "$dW_{k} = \\frac {\\partial L}{\\partial A_{k}} \\cdot \\frac {\\partial A_{k}}{\\partial Z_{k}} \\cdot \\frac {\\partial Z_{k}}{\\partial W_{k}} $\n",
    "\n",
    "$dB_{k} = \\frac {\\partial L}{\\partial A_{k}} \\cdot \\frac {\\partial A_{k}}{\\partial Z_{k}} \\cdot \\frac {\\partial Z_{k}}{\\partial B_{k}}$\n",
    "\n",
    "This is when out_grad() is used\n",
    "\n",
    "#### In order to update the W and B of the Layer, we use step()\n",
    "Since each layer has its own W and B matrices, and we store dW and dB, we can update W and B for each matrix as\n",
    "\n",
    "$W = W - $ learning_rate $\\cdot dW$\n",
    "\n",
    "$B = B - $ learning_rate $\\cdot dB$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self, n_prev, n_next, activation):\n",
    "        #Each layer object has  W, B and activation()\n",
    "        self.W = init_theta(n_prev, n_next, activation)\n",
    "        self.B = init_theta(1, n_next, activation)\n",
    "        self.activation = activation()\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        #A layer first computes Z = W.A_prev + B and then A = activation(Z)\n",
    "        #Compute Z, remember this class has a W,B and activation(), and how to call member functions \n",
    "        #and member functions of member objects. Also remember to use matrix operations for the same.  \n",
    "        \n",
    "        self.Z =  ''' INSERT CODE HERE '''\n",
    "        self.A = ''' INSERT CODE HERE '''\n",
    "        return self.A\n",
    "    \n",
    "    def grad(self, dL_dZ_next, W_next, A_prev, m):\n",
    "        #Compute dL_dA,   \n",
    "        dL_dA = ''' INSERT CODE HERE '''\n",
    "        \n",
    "        #Compute dA_dZ i.e the differential of this layer's activation w.r.t this layer's Z\n",
    "        dA_dZ = ''' INSERT CODE HERE '''\n",
    "        \n",
    "        #Compute dZ i.e dL_dZ of this layer. \n",
    "        self.dZ = ''' INSERT CODE HERE '''\n",
    "        \n",
    "        #Compute dW and dB, with the same shape as W and B respectively\n",
    "        self.dW = ''' INSERT CODE HERE '''\n",
    "        self.dB = ''' INSERT CODE HERE '''\n",
    "    \n",
    "    def out_grad(self, dL_dZ, A_prev, m):\n",
    "        self.dZ = dL_dZ\n",
    "        self.dW = (1./m)*(np.dot(self.dZ, A_prev.T))\n",
    "        self.dB = (1./m)*(np.sum(self.dZ, axis=1, keepdims=True))\n",
    "        \n",
    "    def step(self, lr):\n",
    "        # lr stands for learning rate.\n",
    "        #Compute the step updates on W and B, using this layer's dW, dB, and lr.\n",
    "        self.W = ''' INSERT CODE HERE '''\n",
    "        self.B = ''' INSERT CODE HERE '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "NNs can be quite tricky to optimize because of the sheer number of parameters. Optimization can quickly get out of hand. A mini-batch gradient descent method is empirically shown to perform best and with decent stability. This is also called stochastic gradient descent interchangeably.\n",
    "\n",
    "There are modifications to SGD that can be done such as Momentum, RMS_Prop, Adam etc. which are not included for sake of simplicity. This works well enough for relatively shallow networks. When Deep NNs are used, these more advanced optimizers become handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(batch_size,X,Y,model,lr=0.001):\n",
    "    m = np.shape(X)[1]\n",
    "    \n",
    "    for i in range(0,m,batch_size):\n",
    "        X_batch = X[:,i:i+batch_size]\n",
    "        Y_batch = Y[:,i:i+batch_size]\n",
    "        \n",
    "        #call model's f_pass() on X_batch\n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        #call model's back_prop() for X_batch,Y_batch and batch_size\n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        #call model's optim() for lr\n",
    "        ''' INSERT CODE HERE '''\n",
    "    \n",
    "    return model.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the optimization problem: training.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        Take a data point and go through the forward pass. \n",
    "    </li>\n",
    "    <li>\n",
    "        Compute the loss function. This is a function of the actual label $Y$ and \n",
    "        predicted label $H$, $f(Y, H)$. It captures how far off our predictions \n",
    "        are from the actual target.\n",
    "    </li>\n",
    "    <li>\n",
    "        Backward Propagation. In this step, we calculate the gradients of the loss \n",
    "        function $f(Y, H)$ with respect to $A$, $W$, and $b$ called $dA$, $dW$ \n",
    "        and $dB$. Using these gradients we update the values of the parameters from the \n",
    "        last layer to the first.\n",
    "    </li>\n",
    "</ul>\n",
    "Repeat these stepsfor $n$ iterations/epochs till we feel we have minimized the loss function, without overfitting the train data (more on this later!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The NN is finally trained for n_epochs and a plot of the training accuracy and test accuracy over n_epochs is displayed. This helps determine overfit/underfit and tuning of the hyperparameters.\n",
    "\n",
    "Nothing fancy to be done here. We are just calling SGD on our model using the hyperparameters we pass as parameters. We are appyling inv_one_hot transformations in order to be able to calculate the metric we defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, Y, X_test, Y_test, metric, n_epochs=100, batch_size=4, lr=0.01, lr_decay=1):\n",
    "    data_size = X.shape[1]\n",
    "    for e in range(n_epochs):\n",
    "        #shuffle dataset\n",
    "        # shuffling helps remove possible serial-relations between data, and reduces 'memorisation'\n",
    "        np.random.seed(138)\n",
    "        shuffle_index = np.random.permutation(data_size)\n",
    "        X, Y = X[:,shuffle_index], Y[:,shuffle_index]\n",
    "\n",
    "        #SGD\n",
    "        loss = SGD(batch_size,X,Y,model,lr)\n",
    "        \n",
    "        #decay helps decrease the size of steps over time, improving stability and convergence\n",
    "        lr = lr*lr_decay\n",
    "        \n",
    "        #train accuracy\n",
    "        H = model.f_pass(X)\n",
    "        tr_acc = metric(H,Y)\n",
    "        \n",
    "        #test accuracy\n",
    "        H = model.f_pass(X_test)\n",
    "        acc = metric(H,Y_test)\n",
    "\n",
    "        #plot train accuracy and test accuracy vs epochs\n",
    "        plt.plot(e,tr_acc, 'bo')\n",
    "        plt.plot(e,acc,'ro')\n",
    "        clear_output()\n",
    "        print(f\"epoch:{e+1}/{n_epochs} | Loss:{loss:.4f} | \\\n",
    "            Train Accuracy: {tr_acc:.4f} | Test_Accuracy:{acc:.4f}\")\n",
    "        \n",
    "    #plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of what we have done so far:\n",
    "\n",
    "1. Defined Class Layer and we can use objects of this class to stack multiple layers and build a NN.\n",
    "2. Defined Activation functions and loss function as classes. While creating objects of the class Layer in the NN, we can select which Activation we want and what loss we will use for the NN\n",
    "3. Defined function SGD() as the optimizer \n",
    "4. Defined function Train() that executes the training on the dataset \n",
    "\n",
    "\n",
    "### What we need further in order to build the NN:\n",
    "\n",
    "1. Load the dataset for the classification problem we want to solve\n",
    "2. Define a model using the template presented before (under 'Model')\n",
    "3. Specify the values of the Hyperparameters\n",
    "4. Call train() with relevant parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Logistic Regression as a NN\n",
    "\n",
    "Let's repeat the logistic regression exercise on ex2data1.csv as done in week 2, this time based on a NN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data, and partitioning train and test set\n",
    "\n",
    "If you want to find out what happens when the input is not normalized, comment out the normalize function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Train/test split seems to cause a high error - Shuffle df for better results\n",
    "import pandas as pd\n",
    "\n",
    "''' CHANGE PATH IN THE BELOW LINE ACCORDING TO YOUR DIRECTORY STRUCTURE'''\n",
    "#df = pd.read_csv('Datasets/Regression/ex2data1.csv')\n",
    "\n",
    "A2 = df.values.T\n",
    "X1 = A2[0:2,0:-10]\n",
    "X2 = normalize(X1)\n",
    "Y1 = A2[2:3,0:-10]\n",
    "Y2 = one_hot(Y1,2)\n",
    "X1_test = A2[0:2,-10:]\n",
    "X2_test = normalize(X1_test)\n",
    "Y1_test = A2[2:3,-10:]\n",
    "Y2_test = one_hot(Y1_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(data, label_x, label_y, label_pos, label_neg, axes=None):\n",
    "    if axes == None:\n",
    "        axes = plt.gca()\n",
    "    neg = data[:,2] == 0\n",
    "    pos = data[:,2] == 1\n",
    "    axes.scatter(data[pos][:,0], data[pos][:,1], marker='+', c='k', s=60, linewidth=2, label=label_pos)\n",
    "    axes.scatter(data[neg][:,0], data[neg][:,1], c='y', s=60, label=label_neg)\n",
    "    axes.set_xlabel(label_x)\n",
    "    axes.set_ylabel(label_y)\n",
    "    axes.legend(frameon= True, fancybox = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we define the Logistic regression model as a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic:\n",
    "    def __init__(self, X_size, Y_size, lossfn):\n",
    "        self.regressor = layer(X_size, Y_size, softmax)\n",
    "        self.lossfn = lossfn()\n",
    "        \n",
    "    def f_pass(self, X):\n",
    "        #Compute the output of the NN using this model's regressor layer's forward()\n",
    "        self.H = ''' INSERT CODE HERE '''\n",
    "        return self.H\n",
    "    \n",
    "    def back_prop(self, X, Y, batch_size):\n",
    "        m = batch_size\n",
    "        \n",
    "        #Compute loss of model using this model's lossfn's get_loss()\n",
    "        self.loss = ''' INSERT CODE HERE '''\n",
    "        \n",
    "        #Compute dL_dz using this model's lossfn's diff()\n",
    "        dL_dZ = ''' INSERT CODE HERE '''\n",
    "        \n",
    "        #We call out_grad() to calculate the gradients and store them. \n",
    "        self.regressor.out_grad(dL_dZ, X, m)\n",
    "    \n",
    "    def optim(self, lr):\n",
    "        #Call this model's \n",
    "        ''' INSERT CODE HERE '''self.regressor.step(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = np.shape(X2)[0]\n",
    "n_out = np.shape(Y2)[0]\n",
    "\n",
    "# log_reg is the object of the class Logistic, which is a 'model'\n",
    "log_reg = Logistic(n_in,n_out,CE_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Parameters that define the training and the NN are called Hyperparameters ( so Meta!)\n",
    "These include the learning rate, the batch size for the SGD, the decay in the learning rate etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters feel free to tweak these!\n",
    "\n",
    "#batch_size\n",
    "#lies in [1,X.shape[1]], preferably powers of 2\n",
    "# 1 -> stochastic descent\n",
    "# X.shape[1] -> batch descent\n",
    "batch_size = 16\n",
    "\n",
    "#learning rate\n",
    "lr = 0.01\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "#learning rate decay parameter, lies in [0,1]\n",
    "lr_decay = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following will be passed to train() as parameters\n",
    "# training data : X2, Y2\n",
    "# test data : X2_test and Y2_test\n",
    "# metric : model_accuracy\n",
    "# model : log_ref\n",
    "\n",
    "''' INSERT CODE HERE '''\n",
    "#call train() with parameters \n",
    "\n",
    "#The plot helps visualise train accuracy and test accuracy over epochs, and therefore check how the model is \n",
    "#fitting the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are using log_reg to infer H from X2, and convert it to a label vector representation H2. \n",
    "#A3 combines H2 and A3 vertically so that we can pass it to plotData()\n",
    "\n",
    "H = log_reg.f_pass(X2)\n",
    "H2 = inv_one_hot(H)\n",
    "A3 = np.vstack([X1,H2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(A3.T, 'Exam 1 score', 'Exam 2 score', 'Admitted', 'Not admitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotData(A2.T, 'Exam 1 score', 'Exam 2 score', 'Admitted', 'Not admitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: 3 Layers \n",
    "\n",
    "Now we add two layers to this Logistic-Regression-like NN and try to get similar or better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HL_net:\n",
    "    def __init__(self, X_size, Y_size, lossfn):\n",
    "        self.L1 = layer(X_size, 100, relu)\n",
    "        self.L2 = layer(100, 20, relu)        \n",
    "        self.L3 = layer(20, Y_size, softmax)\n",
    "        self.lossfn = lossfn()\n",
    "        \n",
    "    def f_pass(self, X):\n",
    "        #Compute the activation of each layer using the respective layer's forward()\n",
    "        A1 = ''' INSERT CODE HERE '''\n",
    "        A2 = ''' INSERT CODE HERE '''\n",
    "        A3 = ''' INSERT CODE HERE '''\n",
    "        self.H = A3\n",
    "        return self.H\n",
    "    \n",
    "    def back_prop(self,X,Y, batch_size):\n",
    "        m = batch_size\n",
    "        \n",
    "        #use the model's lossfn's get_loss() to find the loss of the model\n",
    "        self.loss = ''' INSERT CODE HERE '''\n",
    "        \n",
    "        #use the model's lossfn's diff() to find dL_dZ\n",
    "        dL_dZ = ''' INSERT CODE HERE '''\n",
    "        \n",
    "        #Call out_grad() for the last layer. For a 3-layer NN, example is provided below:\n",
    "        #self.L3.out_grad(dL_dZ, self.L2.A, m)\n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        # call each layer's grad() from the penultimate layer to the first layer to perform back_prop\n",
    "        ''' INSERT CODE HERE '''\n",
    "    \n",
    "    def optim(self, lr):\n",
    "        #call each layer's step() to update their respective W and B\n",
    "        ''' INSERT CODE HERE '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = np.shape(X2)[0]\n",
    "n_out = np.shape(Y2)[0]\n",
    "\n",
    "#This is the new 'model', so bin_class will be passed to train()\n",
    "bin_clas = HL_net(n_in,n_out,CE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "n_epochs = 200\n",
    "lr = 0.01\n",
    "lr_decay = 0.99\n",
    "batch_size = 16 # mini-batch gradient descent here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training data : X2, Y2\n",
    "# test data : X2_test and Y2_test\n",
    "# metric : model_accuracy\n",
    "# model : bin_clas\n",
    "\n",
    "''' INSERT CODE HERE '''\n",
    "#call train() with parameters \n",
    "\n",
    "#The plot helps visualise train accuracy and test accuracy over epochs, and therefore check how the model is \n",
    "#fitting the train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how training accuracy is way higher than test accuracy?\n",
    "\n",
    "This is mainly because of overfitting, and partially because we have such a small dataset over which we repeat training.\n",
    "\n",
    "The act of stopping training at a relatively high training accuracy such that the test accuracy does not decrease further w.r.t the train accuracy is called early stopping and this controls overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Let's Classify images!\n",
    "\n",
    "We finally get to define the NN below. We are free to choose the number of hidden layers and the respective sizes, the activation functions for each of these, the loss function etc. This is where the object oriented setup helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "''' CHANGE PATH IN THE BELOW LINE ACCORDING TO YOUR DIRECTORY STRUCTURE'''\n",
    "#X = read_idx('../Deep_Learning/MNIST_np/data/MNIST/train/train-images-idx3-ubyte')     \n",
    "X = flatten_imgs(X)\n",
    "X = normalize(X)               # Try without this\n",
    "X = np.transpose(X)\n",
    "\n",
    "''' CHANGE PATH IN THE BELOW LINE ACCORDING TO YOUR DIRECTORY STRUCTURE'''\n",
    "#Y = read_idx('../Deep_Learning/MNIST_np/data/MNIST/train/train-labels-idx1-ubyte')\n",
    "Y = np.expand_dims(Y, axis=1)\n",
    "Y = np.transpose(Y)\n",
    "Y = one_hot(Y,10)\n",
    "\n",
    "''' CHANGE PATH IN THE BELOW LINE ACCORDING TO YOUR DIRECTORY STRUCTURE'''\n",
    "#X_test = read_idx('../Deep_Learning/MNIST_np/data/MNIST/test/t10k-images.idx3-ubyte')\n",
    "X_test = flatten_imgs(X_test)\n",
    "X_test = normalize(X_test)     # Try without this\n",
    "X_test = np.transpose(X_test)\n",
    "\n",
    "''' CHANGE PATH IN THE BELOW LINE ACCORDING TO YOUR DIRECTORY STRUCTURE'''\n",
    "#Y_test = read_idx('../Deep_Learning/MNIST_np/data/MNIST/test/t10k-labels.idx1-ubyte')\n",
    "Y_test = np.expand_dims(Y_test, axis=1)\n",
    "Y_test = np.transpose(Y_test)\n",
    "Y_test = one_hot(Y_test,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualise what one sample of our dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 8\n",
    "plt.imshow(X[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(Y[:,i])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_net:\n",
    "    def __init__(self, X_size, Y_size, lossfn):\n",
    "        \n",
    "        # Build the network. Recommended depth: 2-5 layers. Number of nodes: below 200 preferably.\n",
    "        # Final activation should be softmax, since each datapoint belongs to only one clas\n",
    "        \n",
    "        # Possible code provided below (You could try and tweak this first)\n",
    "        \n",
    "        #self.L1 = layer(X_size, 150, relu)\n",
    "        #self.L2 = layer(150, 50, relu)        \n",
    "        #self.L3 = layer(50, Y_size, softmax)        \n",
    "        #self.lossfn = lossfn()\n",
    "        \n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "    def f_pass(self, X):\n",
    "        #call each layer's forward function like we did in HL_net\n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        # self.H =  \"layername\".forward() of last layer\n",
    "        self.H = ''' INSERT CODE HERE '''\n",
    "        \n",
    "        return self.H\n",
    "    \n",
    "    def back_prop(self,X,Y, batch_size):\n",
    "        m = batch_size\n",
    "        \n",
    "        #use the model's lossfn's get_loss() to find the loss of the model\n",
    "        self.loss = ''' INSERT CODE HERE '''\n",
    "        \n",
    "        #use the model's lossfn's diff() to find dL_dZ\n",
    "        dL_dZ = ''' INSERT CODE HERE '''\n",
    "        \n",
    "        #Call out_grad() for the last layer. For a 3-layer NN, example is provided below:\n",
    "        #self.L3.out_grad(dL_dZ, self.L2.A, m)\n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "        # call each layer's grad() from the penultimate layer to the first layer to perform back_prop\n",
    "        ''' INSERT CODE HERE '''\n",
    "        \n",
    "    def optim(self, lr):\n",
    "        #call each layer's step() to update their respective W and B\n",
    "        ''' INSERT CODE HERE '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "n_in = np.shape(X)[0]\n",
    "n_out = np.shape(Y)[0]\n",
    "mnist_net = MNIST_net(n_in,n_out,CE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters feel free to tweak these!\n",
    "\n",
    "#batch_size\n",
    "#lies in [1,X.shape[1]], preferably powers of 2\n",
    "# 1 -> stochastic descent\n",
    "# X.shape[1] -> batch descent\n",
    "batch_size = 16\n",
    "\n",
    "#learning rate\n",
    "lr = 0.01\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "#learning rate decay parameter, lies in [0,1]\n",
    "lr_decay = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data : X, Y\n",
    "# test data : X_test and Y_test\n",
    "# metric : model_accuracy\n",
    "# model : mnist_net\n",
    "\n",
    "#call train() with parameters \n",
    "''' INSERT CODE HERE '''\n",
    "\n",
    "\n",
    "#The plot helps visualise train accuracy and test accuracy over epochs, and therefore check how the model is \n",
    "#fitting the train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: The empirical process of training NNs\n",
    "\n",
    "We have defined a 3-layer NN to classify handwritten digits.\n",
    "\n",
    "Play around with the depth of the NN (add more layers), the width of the layers (add more nodes), the hyperparameters, the activation functions and observe the effects these bear on the performance of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = np.shape(X)[0]\n",
    "n_out = np.shape(Y)[0]\n",
    "\n",
    "#We create a fresh object of MNIST_net\n",
    "mnist_net = MNIST_net(n_in,n_out,CE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "lr = 0.01\n",
    "lr_decay = 0.99\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training data : X, Y\n",
    "# test data : X_test and Y_test\n",
    "# metric : model_accuracy\n",
    "# model : mnist_net\n",
    "\n",
    "#call train() with parameters \n",
    "''' INSERT CODE HERE '''\n",
    "\n",
    "#The plot helps visualise train accuracy and test accuracy over epochs, and therefore check how the model is \n",
    "#fitting the train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your observations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onlinepub",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
