{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-armed Bandits\n",
    "\n",
    "Choose from K different actions repeatedly and receive a numerical reward chosen from a stationary probability distribution conditional on the selected action each time independent of the previous action(s). \n",
    "\n",
    "In the K-armed Bandit problem, each action $a$ has an expected reward $q_*(a)$. This is called the value of that action.\n",
    "\n",
    "$  q_*(a) \\doteq \\mathbb E[R_t | A_t=a] $\n",
    "\n",
    "Knowing the value of each action would enable us to solve the problem by simply selecting the highest-valued action always. However, it is likely we will have only estimates of the action values at any time, $Q_t(a)$. \n",
    "The objective is to get $Q_t(a)$ as close to $q_t(a)$ as possible.\n",
    "\n",
    "### Action-value Methods\n",
    "\n",
    "One way to estimate $Q_t(a)$ is to average rewards over the timesteps\n",
    "\n",
    "$Q_t(a) \\doteq \\frac{\\sum^{t-1}_{i=1}R_i \\cdot \\mathbb 1_{A_i=a}}{\\sum^{t-1}_{i=1}\\mathbb 1_{A_i=a}}$\n",
    "\n",
    "The best way to select an action, a.k.a the policy would be 'greedy':\n",
    "\n",
    "$A_t \\doteq \\underset{a}{argmax} Q_t(a)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Value Function\n",
    "$V^{\\pi}(s) = \\underset{s'}{\\sum} P(s'|s,a)( R(s,a,s')  + \\gamma V^{\\pi}(s')) $ \n",
    "\n",
    "## Action Value Function\n",
    "$Q^{\\pi}(s) = \\underset{s'}{\\sum} P(s'|s,a)( R(s,a,s')  + \\gamma V^{\\pi}(s')) $ \n",
    "\n",
    "## Optimal Value Function\n",
    "$V^{*}(s) = \\underset{a}{max} \\underset{s'}{\\sum} P(s'|s,a)( R(s,a,s')  + \\gamma V^{*}(s')) $ \n",
    "\n",
    "## Optimal Action Value Function\n",
    "$Q^{*}(s,a) = \\underset{s'}{\\sum} P(s'|s,a)( R(s,a,s')  + \\gamma \\cdot \\underset{a}{max} \\ Q^{*}(s',a')) $ \n",
    "\n",
    "k is timesteps from horizon\n",
    "\n",
    "## Dynamic Programming \n",
    "\n",
    "The term dynamic programming (DP) refers to a collection of algorithms that can be\n",
    "used to compute optimal policies given a perfect model of the environment as a Markov\n",
    "decision process (MDP).\n",
    "\n",
    "### Policy Evaluation\n",
    "\n",
    "$V^{\\pi}_{k}(s) \\leftarrow \\underset{a}{\\sum}\\pi(a|s) \\underset{s'}{\\sum} P(s'|s,\\pi (s))(R(s,\\pi(s),s')+ \\gamma V^{\\pi}_{k-1}(s'))$\n",
    "\n",
    "### Policy Improvement\n",
    "\n",
    "$\\pi_{k+1}(s) \\leftarrow \\underset{a}{argmax}\\ q^{\\pi}(s,a)$\n",
    "\n",
    "Where $ q^{\\pi}(s,a) = \\underset{s'}{\\sum} P(s'|s,a)(R(s,\\pi(s),s')+ \\gamma V^{\\pi}(s'))$\n",
    "\n",
    "### Policy Iteration\n",
    "\n",
    "1. Initialize Policy $\\pi$\n",
    "\n",
    "2. Evaluate: Until $V^{\\pi}_{k}(s) - V^{\\pi}_{k+1}(s) < $ tolerance: For all  $s$ in $\\mathbb S$ , evalute $\\pi$ as $V^{\\pi}_{k}(s)$\n",
    "3. Improve: If $\\pi_{k}(s) \\ \\ != \\pi_{k+1}(s) <$ perform policy improvement using $V^{\\pi}_{k}(s)$\n",
    "\n",
    "### Value Iteration\n",
    "\n",
    "1. Initialize $V(s)$ for all s in $\\mathbb S $\n",
    "2. Until $V_{k}(s) - V_{k+1}(s) < $ tolerance: For all  $s$ in $\\mathbb S$ , $V_{k}(s)$ as\n",
    "\n",
    "$V_{k}(s) = \\underset{a}{max} \\underset{s}{\\sum} P(s'|s,a)(R(s,a,s')+ \\gamma V(s'))$\n",
    "3. Perform policy improvement using $V_{k}(s)$ such that $ \\pi \\approx \\pi *$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converged(V,V_new, tol=10e-3):\n",
    "    \"\"\"\n",
    "    Checks whether first two arguments have converged to within the third argument, tolerance.\n",
    "    Arguments:\n",
    "        V : Numpy float array\n",
    "        V_new: Numpy float array\n",
    "        tol: float\n",
    "    Returns:\n",
    "        bool True or False\n",
    "    \"\"\"\n",
    "    if np.all(np.abs(V - V_new) < tol) : \n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy, max_episodes=10):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        state,reward,done,info = env.step(policy[state])\n",
    "        print(state,reward,done,info)\n",
    "        env.render()\n",
    "        if done:\n",
    "            env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(pi, P, nS, nA, gamma=0.9, max_iter=1000, tol=10e-3):\n",
    "    V = np.zeros(nS)\n",
    "    V_new = V.copy()\n",
    "    for i in range(max_iter):\n",
    "        V = V_new.copy()\n",
    "        V_new = np.zeros(nS)\n",
    "        for state in range(nS):\n",
    "            for probability, next_state, reward, done in P[state][pi[state]]: \n",
    "                V_new[state] += probability*(reward + gamma*V[next_state])\n",
    "        if converged(V,V_new,tol):\n",
    "            break\n",
    "    return V_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(pi, P, V, nS, nA, gamma=0.9):\n",
    "    pi_new = np.zeros(nS, dtype='int')\n",
    "    for state in range(nS):\n",
    "        B = np.zeros(nA)\n",
    "        q = -99\n",
    "        for action in range(nA):\n",
    "            for probability, next_state, reward, done in P[state][action]: \n",
    "                B[action] += probability*(reward + gamma*V[next_state])\n",
    "            if B[action]>q:\n",
    "                q = B[action]\n",
    "                pi_new[state] = action\n",
    "            elif B[action] == q:\n",
    "                if np.random.random() < 0.5:\n",
    "                    pi_new[state] == action       \n",
    "    return pi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iter=1000, gamma=0.9, tol=10e-3):  \n",
    "    #nS number of States in env\n",
    "    nS = env.nS\n",
    "    #nA number of actions in env\n",
    "    nA = env.nA\n",
    "    # P is dynamics model of env as [state,action] dictionary with (probability, next_state, reward, done) \n",
    "    P = env.P\n",
    "    \n",
    "    V = np.zeros(nS,dtype=float)\n",
    "    pi = np.zeros(nS,dtype=int)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        V_new = np.zeros(nS, dtype=float)\n",
    "        for state in range(nS):\n",
    "            for action in range(nA):\n",
    "                q = 0\n",
    "                for probability, next_state, reward, done in P[state][action]: \n",
    "                    q += probability*(reward + gamma*V[next_state])\n",
    "                if V_new[state] < q:\n",
    "                    V_new[state] = q\n",
    "        if converged(V,V_new,tol):\n",
    "            break\n",
    "        V = V_new.copy()\n",
    "    pi_new = improve_policy(pi,P,V_new,nS,nA,gamma=gamma)\n",
    "    \n",
    "    return V_new, pi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, max_iter=20, gamma=0.9, tol=10e-3):\n",
    "    #nS number of States in env\n",
    "    nS = env.nS\n",
    "    #nA number of actions in env\n",
    "    nA = env.nA\n",
    "    # P is dynamics model of env as [state,action] dictionary with (probability, next_state, reward, done) \n",
    "    P = env.P\n",
    "    \n",
    "    V = np.zeros(nS,dtype=float)\n",
    "    pi = np.zeros(nS,dtype=int)\n",
    "    \n",
    "    pi = np.array([np.random.randint(nA) for i in range(nS)])\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        V_new = evaluate_policy(pi,P,nS,nA,gamma=gamma,tol=tol)\n",
    "        pi_new = improve_policy(pi,P,V_new,nS,nA,gamma=gamma)\n",
    "        \n",
    "        if converged(V,V_new,tol):\n",
    "            break\n",
    "        \n",
    "        V = V_new.copy()\n",
    "        pi=pi_new.copy()\n",
    "    \n",
    "    return V_new, pi_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 2]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_A_E  = [[1,0,2],[2,1,3],[3,4,5]]\n",
    "S_A_E[:2][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[[]]*3]*4\n",
    "a[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_es(env, max_iter = 10, gamma=0.99):\n",
    "    #nS number of States in env\n",
    "    nS = env.nS\n",
    "    #nA number of actions in env\n",
    "    nA = env.nA\n",
    "    # P is dynamics model of env as [state,action] dictionary with (probability, next_state, reward, done) \n",
    "    \n",
    "    # initialize Q abitrarily for all states and actions\n",
    "    Q = np.zeros([nS,nA], dtype=float)\n",
    "    # initialize Returns as an empty list\n",
    "    Returns = [[[]]*nA]*nS\n",
    "    # initialize pi arbitrarily\n",
    "    pi = np.array([np.random.randint(nA) for i in range(nS)])\n",
    "    \n",
    "    #Loop forever (for max iter in order to ensure exit at some point)\n",
    "    for i in range(max_iter):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        S_A_R = []\n",
    "        pi = np.array([np.random.randint(nA) for i in range(nS)])\n",
    "        #Returns = [[[]]*nA]*nS\n",
    "        \n",
    "        # Generate an episode\n",
    "        while not done:\n",
    "            action = pi[state]\n",
    "            next_state, reward, done, __ = env.step(action)        \n",
    "            S_A_R.append([state,action,reward])\n",
    "            state = next_state\n",
    "        G = 0\n",
    "        \n",
    "        # for each step in episode, average\n",
    "        for i in range(len(S_A_R)-1,-1,-1):    \n",
    "            G = gamma*G + S_A_R[i][2]\n",
    "            \n",
    "            #Unless S_t and A_t appears in Episode until t\n",
    "            for S,A,R in S_A_R[:i-1]:\n",
    "                if (S,A,R) == S_A_R[i]:\n",
    "                    break\n",
    "            else:\n",
    "                #if G>0: print(G)\n",
    "                State,Action = S_A_R[i][0:2]\n",
    "                Returns[State][Action].append(G)\n",
    "                Q[State][Action] = np.mean(np.array(Returns[State][Action]))\n",
    "                \n",
    "                #Ensures policy doesn't pick 0 if max Q = 0, avoiding sub-optimal policies.\n",
    "                if np.any(Q[State]>0):\n",
    "                    pi[State] = np.argmax(Q[State])\n",
    "\n",
    "    return Q, pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Methods (TD)\n",
    "\n",
    "### TD(0)\n",
    "\n",
    "\n",
    "### SARSA\n",
    "Basically on policy Q Learning\n",
    "2.4 is here:  $ target \\leftarrow R + \\gamma \\cdot Q(S',A') $\n",
    "where A' is derived from policy, and is hte next action to be taken in S'\n",
    "\n",
    "### Q Learning\n",
    "1. Intialize Q for each (s,a) pair arbitrarily.\n",
    "2. While not converged:\n",
    "    2.1 choose A from S using pi derived from Q (e.g eps_greedy)\n",
    "    \n",
    "    2.2 Take action A, observe R,S'\n",
    "    \n",
    "    2.3 If S' is terminal: $ target \\leftarrow R $\n",
    "    \n",
    "    2.4 Else: $ target \\leftarrow R + \\gamma \\cdot max_{a}Q(S',a) $\n",
    "    \n",
    "    2.5 $ Q(S,A) \\leftarrow (1-\\alpha) \\cdot Q(S,A) +\\alpha \\cdot target $  \n",
    "    \n",
    "    2.6 $ S \\leftarrow S'$\n",
    "\n",
    "### Double Q Learning\n",
    "1. Intialize Q for each (s,a) pair arbitrarily.\n",
    "2. While not converged:\n",
    "    2.1 choose A from S using pi derived from Q (e.g eps_greedy)\n",
    "    \n",
    "    2.2 Take action A, observe R,S'\n",
    "    \n",
    "    2.3 If S' is terminal: $ target \\leftarrow R $\n",
    "    \n",
    "    2.4 Else: $ target \\leftarrow R + \\gamma \\cdot max_{a}Q(S',a) $\n",
    "    \n",
    "    2.5 $ Q(S,A) \\leftarrow (1-\\alpha) \\cdot Q(S,A) +\\alpha \\cdot target $  \n",
    "    \n",
    "    2.6 $ S \\leftarrow S'$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30201389610579377"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy(env, Q):\n",
    "    pi = np.zeros(env.nS, dtype=int)\n",
    "    for s in range(env.nS):\n",
    "        pi[s] = np.argmax(Q[s])\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(env, Q_values, eps=0.5):\n",
    "    if eps < np.random.random():\n",
    "        a = np.argmax(Q_values)\n",
    "    else:\n",
    "        a = np.random.randint(env.nA)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, lr=0.8, gamma=.95, t_steps=100000):\n",
    "\n",
    "    #create lists to contain total rewards and steps per episode\n",
    "    Q = np.zeros([env.nS,env.nA])\n",
    "    #jList = []\n",
    "    \n",
    "    d = False\n",
    "    rList = []\n",
    "    s = env.reset()\n",
    " \n",
    "    for i in range(t_steps):\n",
    "        \n",
    "        #a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        a = eps_greedy(env, Q[s], 0.5 - i/t_steps)\n",
    "        #Get new state and reward from environment\n",
    "        \n",
    "        s1,r,d,_ = env.step(a)\n",
    "        \n",
    "        if d == True:\n",
    "            target = r\n",
    "            s1 = env.reset()\n",
    "        else:\n",
    "            target = (r + gamma*np.max(Q[s1]))\n",
    "\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = (1-lr)*Q[s,a] + lr*target\n",
    "\n",
    "        s = s1\n",
    "\n",
    "    pi_Q = get_greedy(env, Q)\n",
    "    return Q, pi_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_ql(env, lr=0.99, gamma=0.9, t_steps=100000):\n",
    "    #create 2 Q lists to contain total rewards and steps per episode\n",
    "    Q1 = np.zeros([env.nS,env.nA])\n",
    "    Q2 = np.zeros([env.nS,env.nA])\n",
    "    \n",
    "    d = False\n",
    "    s = env.reset()\n",
    " \n",
    "    for i in range(t_steps):\n",
    "        #a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        a = eps_greedy(env, Q1[s] + Q2[s], 0.5- i/t_steps)\n",
    "        #Get new state and reward from environment\n",
    "        \n",
    "        s1,r,d,_ = env.step(a)\n",
    "        \n",
    "        prob = np.random.random()\n",
    "        \n",
    "        if d == True:\n",
    "            target1 = r\n",
    "            target2 = r\n",
    "            s1 = env.reset()\n",
    "        else:\n",
    "            target1 = r + gamma*Q2[s1,np.argmax(Q1[s1])]\n",
    "            target2 = r + gamma*Q1[s1,np.argmax(Q2[s1])] \n",
    "        \n",
    "        if prob <=0.5:\n",
    "            Q1[s,a] = (1-lr)*Q1[s,a] + lr*target1\n",
    "        \n",
    "        else:\n",
    "            Q2[s,a] = (1-lr)*Q2[s,a] + lr*target2\n",
    "\n",
    "        s = s1\n",
    "\n",
    "    pi_Q = get_greedy(env, Q1+Q2)\n",
    "    return (Q1+Q2)/2, pi_Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q, pi_Q = q_learning(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQ, pi_DQ = double_ql(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['left', 'up', 'left', 'up'],\n",
       "       ['left', 'left', 'left', 'left'],\n",
       "       ['up', 'down', 'left', 'left'],\n",
       "       ['left', 'right', 'down', 'left']], dtype='<U5')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_directions(pi_VI,(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['left', 'up', 'down', 'up'],\n",
       "       ['left', 'left', 'left', 'left'],\n",
       "       ['up', 'down', 'left', 'left'],\n",
       "       ['left', 'right', 'down', 'left']], dtype='<U5')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_directions(pi_Q,(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['left', 'up', 'right', 'left'],\n",
       "       ['left', 'left', 'left', 'left'],\n",
       "       ['up', 'down', 'left', 'left'],\n",
       "       ['left', 'right', 'down', 'left']], dtype='<U5')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_directions(pi_DQ,(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "9 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "13 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "9 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "13 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "13 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "13 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "13 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "13 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "9 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "10 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "9 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "4 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "9 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "10 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "14 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "14 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "14 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "15 1.0 True {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "play(env,pi_DQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "#env = gym.make(\"FrozenLake8x8-v0\")\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_PI, pi_PI = policy_iteration(env,300,0.99)\n",
    "V_VI, pi_VI = value_iteration(env,2000,0.99)\n",
    "\n",
    "# will take too many episodes because reward is super-sparse i.e given only at one state. \n",
    "#Prob of getting to this state by random actions is too low.\n",
    "#Q_MC_ES, pi_MC_ES = monte_carlo_es(env,8000,.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V_VI.reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V_PI.reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.967e-01, 3.153e-01, 3.074e-01, 3.105e-01],\n",
       "       [7.137e-02, 4.866e-02, 6.238e-02, 3.034e-01],\n",
       "       [3.476e-01, 1.564e-01, 3.100e-01, 3.373e-01],\n",
       "       [9.033e-05, 3.471e-01, 4.512e-02, 3.423e-01],\n",
       "       [4.137e-01, 2.828e-01, 3.188e-01, 1.932e-05],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "       [8.692e-02, 4.980e-01, 7.663e-02, 7.290e-02],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "       [8.176e-02, 1.166e-01, 1.799e-02, 3.926e-01],\n",
       "       [5.614e-01, 5.843e-01, 5.742e-01, 2.279e-02],\n",
       "       [4.780e-01, 1.209e-01, 4.219e-01, 1.062e-01],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "       [6.934e-01, 1.575e-01, 6.952e-01, 1.064e-03],\n",
       "       [6.993e-01, 7.116e-01, 6.344e-01, 9.200e-01],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00]])"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.029, 0.029, 0.029, 0.029],\n",
       "       [0.029, 0.029, 0.029, 0.029],\n",
       "       [0.029, 0.029, 0.029, 0.029],\n",
       "       [0.029, 0.029, 0.029, 0.029],\n",
       "       [0.029, 0.029, 0.029, 0.029],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.029, 0.029, 0.029, 0.029],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.029, 0.029, 0.029, 0.029],\n",
       "       [0.029, 0.029, 0.029, 0.029],\n",
       "       [0.029, 0.029, 0.029, 0.029],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.029, 0.029, 0.029, 0.029],\n",
       "       [0.029, 0.029, 0.029, 0.028],\n",
       "       [0.   , 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_MC_ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 2, 1],\n",
       "       [0, 2, 2, 0],\n",
       "       [3, 2, 3, 2],\n",
       "       [3, 0, 3, 1]])"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_MC_ES.reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 0, 3],\n",
       "       [0, 0, 0, 0],\n",
       "       [3, 1, 0, 0],\n",
       "       [0, 2, 1, 0]])"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_VI.reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directions(pi,shape):\n",
    "    actions = ['left','down','right','up']\n",
    "    policy = np.array([actions[int(i)] for i in pi])\n",
    "    return policy.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#play(env,pi_MC_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_directions(pi_VI,(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_directions(pi_QL,(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play(env,pi_QL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
